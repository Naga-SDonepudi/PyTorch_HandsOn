{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Computer Vision using PyTorch"
      ],
      "metadata": {
        "id": "zJcsUcFVkXRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries\n",
        "* **torchvision** : base domain library with pre trained models for computer vision (consists of popular datasets, model architectures, and common image transformations for computer vision)\n",
        "* Another different libraries are:\n",
        "  * **torchvision.dataset**,\n",
        "  * **torchvision.models**,\n",
        "  * **torchvision.transforms**,\n",
        "  * **torch.utils.data.Dataset**,\n",
        "  * **torch.utils.data.DataLoader**"
      ],
      "metadata": {
        "id": "xsjooF4YkzN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing torch and torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "## Importing matplotlib for visualizing and plotting\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-onzPj2emJxX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL9rPuNoo-Gh",
        "outputId": "d638a6cc-d8e4-48bf-e78f-1e8ed6d01690"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.24.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aim is to build a computer vision model to find patterns in training data, and use the model to predict on test data\n",
        "### 1. Loading a Dataset\n",
        "* Importing a FashionMNIST dataset from torchvision.datasets\n",
        "* The datasets found in torchvision.datasets are splitted to training and test sets"
      ],
      "metadata": {
        "id": "HJLlmTu9pKWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Train Data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiSrciuesBs9",
        "outputId": "30ba3bdd-6164-44ae-bc95-008e7cfd481f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.2MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 172kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.79MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bA2xJUBeldXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYfxCDjhkFIq",
        "outputId": "faea1e2b-628e-492f-8d8a-6c77e83c4254"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh5v5cbLkiwa",
        "outputId": "51b130dc-01a2-44ad-c01d-3735cd4e47fa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiiUpP3PmvrG",
        "outputId": "ae1504eb-c204-482c-e138-4b2f364d465b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 9)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visulazing the data"
      ],
      "metadata": {
        "id": "7A-xXZibnK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap=\"grey\")\n",
        "plt.title(class_names[label]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Cfd-DLHXp_Nx",
        "outputId": "1a7b5ccf-d943-4539-ee46-048344ecc1e0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape: torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKJhJREFUeJzt3Xt0VeWdxvHnJCSHQJLDJeRWAgk3YeSigxAjco9AtAwUrHhZs6CDWpnQFtCxi5lW6rRrUrFjWVQqttMF1okizuJSXUqHi4QqIAVh0BllCAYBQ8Kl5iQk5ELyzh8sz3i4hXeb5E3C97PWXnL2eX/ZLy87edw5+/yOzxhjBABAC4twPQEAwI2JAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIaMWfOHMXGxjY6bty4cRo3blyTHXfcuHEaPHhwk309oLUhgNAu/frXv5bP51NmZqbrqbRJ//Iv/6INGza4ngbaOQII7VJ+fr7S09O1Z88eFRYWup5Om0MAoSUQQGh3ioqKtHPnTj333HPq0aOH8vPzXU8JwBUQQGh38vPz1bVrV91zzz269957rxhAR48elc/n0y9+8Qv95je/Ud++feX3+zVixAj9+c9/bvQYBw4cUI8ePTRu3DidO3fuquNqamq0ZMkS9evXT36/X2lpaXryySdVU1Nz3X+fffv26Y477lBMTIwyMjK0cuXKy8acOnVKc+fOVVJSkjp27Khhw4bppZdeumxcZWWlHn/8caWlpcnv9+umm27SL37xC321Kb7P51NlZaVeeukl+Xw++Xw+zZkz57rnC1w3A7QzAwcONHPnzjXGGLNjxw4jyezZsydsTFFRkZFkbr31VtOvXz/zzDPPmKVLl5qEhATTs2dPU1tbGxo7e/Zs07lz59DjPXv2mK5du5q77rrLVFVVhfaPHTvWjB07NvS4vr7eTJo0yXTq1MksWLDAvPjii2b+/PmmQ4cOZtq0aY3+PcaOHWtSU1NNYmKimT9/vlm+fLm58847jSTzu9/9LjSuqqrKDBo0yERFRZmFCxea5cuXm9GjRxtJZtmyZaFxDQ0NZsKECcbn85mHH37YPP/882bq1KlGklmwYEFo3Msvv2z8fr8ZPXq0efnll83LL79sdu7c2fjCA5YIILQre/fuNZLM5s2bjTEXf+j27NnT/OAHPwgb92UAde/e3fzlL38J7d+4caORZN54443Qvq8G0Lvvvmvi4+PNPffcY6qrq8O+5qUB9PLLL5uIiAjzpz/9KWzcypUrjSTz3nvvXfPvMnbsWCPJ/Ou//mtoX01NjbnllltMYmJiKCSXLVtmJJl///d/D42rra01WVlZJjY21pSXlxtjjNmwYYORZH72s5+FHefee+81Pp/PFBYWhvZ17tzZzJ49+5rzA74ufgWHdiU/P19JSUkaP368pIu/Tpo1a5bWrFmj+vr6y8bPmjVLXbt2DT0ePXq0JOnTTz+9bOw777yjyZMna+LEiVq3bp38fv815/L6669r0KBBGjhwoM6cORPaJkyYEPp6jenQoYO++93vhh5HR0fru9/9rk6dOqV9+/ZJkt566y0lJyfrgQceCI2LiorS97//fZ07d04FBQWhcZGRkfr+978fdozHH39cxhi9/fbbjc4HaEoEENqN+vp6rVmzRuPHj1dRUZEKCwtVWFiozMxMlZaWauvWrZfV9OrVK+zxl2H0xRdfhO2vrq7WPffco1tvvVVr165VdHR0o/M5fPiw/vu//1s9evQI2wYMGCDp4us2jUlNTVXnzp3D9n1Zf/ToUUnSZ599pv79+ysiIvzbedCgQaHnv/xvamqq4uLirjkOaCkdXE8AaCrbtm3TyZMntWbNGq1Zs+ay5/Pz8zVp0qSwfZGRkVf8WuaST6r3+/26++67tXHjRm3atEnf/OY3G51PQ0ODhgwZoueee+6Kz6elpTX6NYD2jABCu5Gfn6/ExEStWLHisufWrVun9evXa+XKlYqJibH+2j6fT/n5+Zo2bZq+/e1v6+23326060Hfvn31X//1X5o4caJ8Pp/1MSWpuLhYlZWVYVdB//u//ytJSk9PlyT17t1bBw8eVENDQ9hV0CeffBJ6/sv/btmyRRUVFWFXQZeO+/LvCzQ3fgWHduH8+fNat26dvvnNb+ree++9bJs/f74qKir0hz/8wfMxoqOjtW7dOo0YMUJTp07Vnj17rjn+vvvu0+eff67f/va3V5xvZWVlo8e8cOGCXnzxxdDj2tpavfjii+rRo4eGDx8uSbr77rtVUlKi1157LazuV7/6lWJjYzV27NjQuPr6ej3//PNhx/jlL38pn8+nnJyc0L7OnTurrKys0fkBXwdXQGgX/vCHP6iiokJ/8zd/c8Xnb7/99tCbUmfNmuX5ODExMXrzzTc1YcIE5eTkqKCg4Kr92v72b/9Wa9eu1WOPPaZ33nlHo0aNUn19vT755BOtXbtWf/zjH3Xbbbdd83ipqal65plndPToUQ0YMECvvfaaDhw4oN/85jeKioqSJD366KN68cUXNWfOHO3bt0/p6en6j//4D7333ntatmxZ6Gpn6tSpGj9+vP7pn/5JR48e1bBhw/Sf//mf2rhxoxYsWKC+ffuGjjt8+HBt2bJFzz33nFJTU5WRkUFbIzQ917fhAU1h6tSppmPHjqaysvKqY+bMmWOioqLMmTNnQrdhP/vss5eNk2SWLFkSenzp+4CMMebMmTPmr/7qr0xycrI5fPiwMeby27CNuXg79DPPPGNuvvlm4/f7TdeuXc3w4cPN008/bYLB4DX/TmPHjjU333yz2bt3r8nKyjIdO3Y0vXv3Ns8///xlY0tLS813vvMdk5CQYKKjo82QIUPMqlWrLhtXUVFhFi5caFJTU01UVJTp37+/efbZZ01DQ0PYuE8++cSMGTPGxMTEGEncko1m4TPmkldbAQBoAbwGBABwggACADhBAAEAnCCAAABOEEAAACcIIACAE63ujagNDQ0qLi5WXFwc7UAAoA0yxqiiokKpqamXNcn9qlYXQMXFxTRpBIB24Pjx4+rZs+dVn291v4K7tFU8AKBtauznebMF0IoVK5Senq6OHTsqMzOz0caNX+LXbgDQPjT287xZAui1117TokWLtGTJEn3wwQcaNmyYJk+efF0fwAUAuEE0R4O5kSNHmtzc3NDj+vp6k5qaavLy8hqtDQaDRhIbGxsbWxvfGmu42+RXQLW1tdq3b5+ys7ND+yIiIpSdna1du3ZdNr6mpkbl5eVhGwCg/WvyADpz5ozq6+uVlJQUtj8pKUklJSWXjc/Ly1MgEAht3AEHADcG53fBLV68WMFgMLQdP37c9ZQAAC2gyd8HlJCQoMjISJWWlobtLy0tVXJy8mXj/X6//H5/U08DANDKNfkVUHR0tIYPH66tW7eG9jU0NGjr1q3Kyspq6sMBANqoZumEsGjRIs2ePVu33XabRo4cqWXLlqmyslLf+c53muNwAIA2qFkCaNasWTp9+rSeeuoplZSU6JZbbtGmTZsuuzEBAHDj8hljjOtJfFV5ebkCgYDraQAAvqZgMKj4+PirPu/8LjgAwI2JAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAONHB9QSA1sTn81nXGGOaYSaXi4uLs6658847PR3r7bff9lRny8t6R0ZGWtdcuHDBuqa187J2XjXXOc4VEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QTNS4CsiIuz/n6y+vt66pl+/ftY1Dz/8sHXN+fPnrWskqbKy0rqmurraumbPnj3WNS3ZWNRLw08v55CX47TkOtg2gDXGqKGhodFxXAEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBM0IwW+wrbpouStGemECROsa7Kzs61rTpw4YV0jSX6/37qmU6dO1jV33XWXdc2//du/WdeUlpZa10gXm2ra8nI+eBEbG+up7nqahF6qqqrK07EawxUQAMAJAggA4ESTB9BPfvIT+Xy+sG3gwIFNfRgAQBvXLK8B3XzzzdqyZcv/H6QDLzUBAMI1SzJ06NBBycnJzfGlAQDtRLO8BnT48GGlpqaqT58+euihh3Ts2LGrjq2pqVF5eXnYBgBo/5o8gDIzM7V69Wpt2rRJL7zwgoqKijR69GhVVFRccXxeXp4CgUBoS0tLa+opAQBaoSYPoJycHH3729/W0KFDNXnyZL311lsqKyvT2rVrrzh+8eLFCgaDoe348eNNPSUAQCvU7HcHdOnSRQMGDFBhYeEVn/f7/Z7e9AYAaNua/X1A586d05EjR5SSktLchwIAtCFNHkBPPPGECgoKdPToUe3cuVPf+ta3FBkZqQceeKCpDwUAaMOa/FdwJ06c0AMPPKCzZ8+qR48euvPOO7V792716NGjqQ8FAGjDmjyA1qxZ09RfEmgxtbW1LXKcESNGWNekp6db13hpripJERH2vxz54x//aF1z6623WtcsXbrUumbv3r3WNZL04YcfWtd8/PHH1jUjR460rvFyDknSzp07rWt27dplNd4Yc11vqaEXHADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA40ewfSAe44PP5PNUZY6xr7rrrLuua2267zbrmah9rfy2dO3e2rpGkAQMGtEjNn//8Z+uaq3245bXExsZa10hSVlaWdc2MGTOsa+rq6qxrvKydJD388MPWNTU1NVbjL1y4oD/96U+NjuMKCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE74jJf2v82ovLxcgUDA9TTQTLx2qW4pXr4ddu/ebV2Tnp5uXeOF1/W+cOGCdU1tba2nY9mqrq62rmloaPB0rA8++MC6xku3bi/rPWXKFOsaSerTp491zTe+8Q1PxwoGg4qPj7/q81wBAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATHVxPADeWVtb7tkl88cUX1jUpKSnWNefPn7eu8fv91jWS1KGD/Y+G2NhY6xovjUVjYmKsa7w2Ix09erR1zR133GFdExFhfy2QmJhoXSNJmzZt8lTXHLgCAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnaEYKfE2dOnWyrvHSfNJLTVVVlXWNJAWDQeuas2fPWtekp6db13hpaOvz+axrJG9r7uV8qK+vt67x2mA1LS3NU11z4AoIAOAEAQQAcMI6gHbs2KGpU6cqNTVVPp9PGzZsCHveGKOnnnpKKSkpiomJUXZ2tg4fPtxU8wUAtBPWAVRZWalhw4ZpxYoVV3x+6dKlWr58uVauXKn3339fnTt31uTJkz198BQAoP2yvgkhJydHOTk5V3zOGKNly5bpRz/6kaZNmyZJ+v3vf6+kpCRt2LBB999//9ebLQCg3WjS14CKiopUUlKi7Ozs0L5AIKDMzEzt2rXrijU1NTUqLy8P2wAA7V+TBlBJSYkkKSkpKWx/UlJS6LlL5eXlKRAIhLbWdIsgAKD5OL8LbvHixQoGg6Ht+PHjrqcEAGgBTRpAycnJkqTS0tKw/aWlpaHnLuX3+xUfHx+2AQDavyYNoIyMDCUnJ2vr1q2hfeXl5Xr//feVlZXVlIcCALRx1nfBnTt3ToWFhaHHRUVFOnDggLp166ZevXppwYIF+tnPfqb+/fsrIyNDP/7xj5Wamqrp06c35bwBAG2cdQDt3btX48ePDz1etGiRJGn27NlavXq1nnzySVVWVurRRx9VWVmZ7rzzTm3atEkdO3ZsulkDANo8n/HS2a8ZlZeXKxAIuJ4GmomXppBeGkJ6ae4oSbGxsdY1+/fvt67xsg7nz5+3rvH7/dY1klRcXGxdc+lrv9fjjjvusK7x0vTUS4NQSYqOjrauqaiosK7x8jPP6w1bXs7xuXPnWo2vr6/X/v37FQwGr/m6vvO74AAANyYCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcsP44BuDr8NJ8PTIy0rrGazfsWbNmWddc7dN+r+X06dPWNTExMdY1DQ0N1jWS1LlzZ+uatLQ065ra2lrrGi8dvuvq6qxrJKlDB/sfkV7+nbp3725ds2LFCusaSbrlllusa7ysw/XgCggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnKAZKVqUl6aGXhpWevXRRx9Z19TU1FjXREVFWde0ZFPWxMRE65rq6mrrmrNnz1rXeFm7jh07WtdI3pqyfvHFF9Y1J06csK558MEHrWsk6dlnn7Wu2b17t6djNYYrIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABw4oZuRurz+TzVeWkKGRFhn/Ve5ldXV2dd09DQYF3j1YULF1rsWF689dZb1jWVlZXWNefPn7euiY6Otq4xxljXSNLp06eta7x8X3hpEurlHPeqpb6fvKzd0KFDrWskKRgMeqprDlwBAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIAT7aYZqZdmfvX19Z6O1dobarZmY8aMsa6ZOXOmdc2oUaOsaySpqqrKuubs2bPWNV4ai3boYP/t6vUc97IOXr4H/X6/dY2XBqZem7J6WQcvvJwP586d83SsGTNmWNe88cYbno7VGK6AAABOEEAAACesA2jHjh2aOnWqUlNT5fP5tGHDhrDn58yZI5/PF7ZNmTKlqeYLAGgnrAOosrJSw4YN04oVK646ZsqUKTp58mRoe/XVV7/WJAEA7Y/1q5o5OTnKycm55hi/36/k5GTPkwIAtH/N8hrQ9u3blZiYqJtuuknz5s275l1CNTU1Ki8vD9sAAO1fkwfQlClT9Pvf/15bt27VM888o4KCAuXk5Fz1dtC8vDwFAoHQlpaW1tRTAgC0Qk3+PqD7778/9OchQ4Zo6NCh6tu3r7Zv366JEydeNn7x4sVatGhR6HF5eTkhBAA3gGa/DbtPnz5KSEhQYWHhFZ/3+/2Kj48P2wAA7V+zB9CJEyd09uxZpaSkNPehAABtiPWv4M6dOxd2NVNUVKQDBw6oW7du6tatm55++mnNnDlTycnJOnLkiJ588kn169dPkydPbtKJAwDaNusA2rt3r8aPHx96/OXrN7Nnz9YLL7yggwcP6qWXXlJZWZlSU1M1adIk/fSnP/XU8wkA0H75jNcufc2kvLxcgUDA9TSaXLdu3axrUlNTrWv69+/fIseRvDU1HDBggHVNTU2NdU1EhLffLtfV1VnXxMTEWNcUFxdb10RFRVnXeGlyKUndu3e3rqmtrbWu6dSpk3XNzp07rWtiY2OtayRvzXMbGhqsa4LBoHWNl/NBkkpLS61rBg0a5OlYwWDwmq/r0wsOAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAATjT5R3K7cvvtt1vX/PSnP/V0rB49eljXdOnSxbqmvr7euiYyMtK6pqyszLpGki5cuGBdU1FRYV3jpcuyz+ezrpGk8+fPW9d46c583333Wdfs3bvXuiYuLs66RvLWgTw9Pd3TsWwNGTLEusbrOhw/fty6pqqqyrrGS0d1rx2+e/fu7amuOXAFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOtNpmpBEREVYNJZcvX259jJSUFOsayVuTUC81XpoaehEdHe2pzsvfyUuzTy8CgYCnOi+NGn/+859b13hZh3nz5lnXFBcXW9dIUnV1tXXN1q1brWs+/fRT65r+/ftb13Tv3t26RvLWCDcqKsq6JiLC/lqgrq7OukaSTp8+7amuOXAFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABO+IwxxvUkvqq8vFyBQEAPPfSQVZNMLw0hjxw5Yl0jSbGxsS1S4/f7rWu88NI8UfLW8PP48ePWNV4aavbo0cO6RvLWFDI5Odm6Zvr06dY1HTt2tK5JT0+3rpG8na/Dhw9vkRov/0Zemop6PZbX5r62bJo1f5WX7/fbb7/danxDQ4M+//xzBYNBxcfHX3UcV0AAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4EQH1xO4mtOnT1s1zfPS5DIuLs66RpJqamqsa7zMz0tDSC+NEK/VLPBa/vKXv1jXfPbZZ9Y1Xtbh/Pnz1jWSVF1dbV1z4cIF65r169db13z44YfWNV6bkXbr1s26xkvDz7KyMuuauro66xov/0bSxaaatrw0+/RyHK/NSL38jBgwYIDV+AsXLujzzz9vdBxXQAAAJwggAIATVgGUl5enESNGKC4uTomJiZo+fboOHToUNqa6ulq5ubnq3r27YmNjNXPmTJWWljbppAEAbZ9VABUUFCg3N1e7d+/W5s2bVVdXp0mTJqmysjI0ZuHChXrjjTf0+uuvq6CgQMXFxZoxY0aTTxwA0LZZ3YSwadOmsMerV69WYmKi9u3bpzFjxigYDOp3v/udXnnlFU2YMEGStGrVKg0aNEi7d++2/lQ9AED79bVeAwoGg5L+/46Zffv2qa6uTtnZ2aExAwcOVK9evbRr164rfo2amhqVl5eHbQCA9s9zADU0NGjBggUaNWqUBg8eLEkqKSlRdHS0unTpEjY2KSlJJSUlV/w6eXl5CgQCoS0tLc3rlAAAbYjnAMrNzdVHH32kNWvWfK0JLF68WMFgMLR5eb8MAKDt8fRG1Pnz5+vNN9/Ujh071LNnz9D+5ORk1dbWqqysLOwqqLS0VMnJyVf8Wn6/X36/38s0AABtmNUVkDFG8+fP1/r167Vt2zZlZGSEPT98+HBFRUVp69atoX2HDh3SsWPHlJWV1TQzBgC0C1ZXQLm5uXrllVe0ceNGxcXFhV7XCQQCiomJUSAQ0Ny5c7Vo0SJ169ZN8fHx+t73vqesrCzugAMAhLEKoBdeeEGSNG7cuLD9q1at0pw5cyRJv/zlLxUREaGZM2eqpqZGkydP1q9//esmmSwAoP3wGWOM60l8VXl5uQKBgIYMGaLIyMjrrvvtb39rfawzZ85Y10hS586drWu6d+9uXeOlUeO5c+esa7w0T5SkDh3sX0L00nSxU6dO1jVeGphK3tYiIsL+Xh4v33aX3l16Pb76JnEbXpq5fvHFF9Y1Xl7/9fJ966WBqeStiamXY8XExFjXXO119cZ4aWKan59vNb6mpkbPP/+8gsHgNZsd0wsOAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAATnj6RNSW8OGHH1qNX7dunfUx/u7v/s66RpKKi4utaz799FPrmurqausaL12gvXbD9tLBNzo62rrGpiv6l2pqaqxrJKm+vt66xktn66qqKuuakydPWtd4bXbvZR28dEdvqXO8trbWukby1pHeS42XDtpeOnVLuuyDRK9HaWmp1fjrXW+ugAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACZ/x2q2wmZSXlysQCLTIsXJycjzVPfHEE9Y1iYmJ1jVnzpyxrvHSCNFL40nJW5NQL81IvTS59DI3SfL5fNY1Xr6FvDSA9VLjZb29HsvL2nnh5Ti2zTS/Di9r3tDQYF2TnJxsXSNJBw8etK657777PB0rGAwqPj7+qs9zBQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAATrTaZqQ+n8+q6aCXZn4tafz48dY1eXl51jVemp56bf4aEWH//y9emoR6aUbqtcGqF6dOnbKu8fJt9/nnn1vXeP2+OHfunHWN1wawtrysXV1dnadjVVVVWdd4+b7YvHmzdc3HH39sXSNJO3fu9FTnBc1IAQCtEgEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcaLXNSNFyBg4c6KkuISHBuqasrMy6pmfPntY1R48eta6RvDWtPHLkiKdjAe0dzUgBAK0SAQQAcMIqgPLy8jRixAjFxcUpMTFR06dP16FDh8LGjBs3LvRZPl9ujz32WJNOGgDQ9lkFUEFBgXJzc7V7925t3rxZdXV1mjRpkiorK8PGPfLIIzp58mRoW7p0aZNOGgDQ9ll91OSmTZvCHq9evVqJiYnat2+fxowZE9rfqVMnJScnN80MAQDt0td6DSgYDEqSunXrFrY/Pz9fCQkJGjx4sBYvXnzNj7WtqalReXl52AYAaP+sroC+qqGhQQsWLNCoUaM0ePDg0P4HH3xQvXv3Vmpqqg4ePKgf/vCHOnTokNatW3fFr5OXl6enn37a6zQAAG2U5/cBzZs3T2+//bbefffda75PY9u2bZo4caIKCwvVt2/fy56vqalRTU1N6HF5ebnS0tK8TAke8T6g/8f7gICm09j7gDxdAc2fP19vvvmmduzY0egPh8zMTEm6agD5/X75/X4v0wAAtGFWAWSM0fe+9z2tX79e27dvV0ZGRqM1Bw4ckCSlpKR4miAAoH2yCqDc3Fy98sor2rhxo+Li4lRSUiJJCgQCiomJ0ZEjR/TKK6/o7rvvVvfu3XXw4EEtXLhQY8aM0dChQ5vlLwAAaJusAuiFF16QdPHNpl+1atUqzZkzR9HR0dqyZYuWLVumyspKpaWlaebMmfrRj37UZBMGALQP1r+Cu5a0tDQVFBR8rQkBAG4MdMMGADQLumEDAFolAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAE60ugIwxrqcAAGgCjf08b3UBVFFR4XoKAIAm0NjPc59pZZccDQ0NKi4uVlxcnHw+X9hz5eXlSktL0/HjxxUfH+9ohu6xDhexDhexDhexDhe1hnUwxqiiokKpqamKiLj6dU6HFpzTdYmIiFDPnj2vOSY+Pv6GPsG+xDpcxDpcxDpcxDpc5HodAoFAo2Na3a/gAAA3BgIIAOBEmwogv9+vJUuWyO/3u56KU6zDRazDRazDRazDRW1pHVrdTQgAgBtDm7oCAgC0HwQQAMAJAggA4AQBBABwggACADjRZgJoxYoVSk9PV8eOHZWZmak9e/a4nlKL+8lPfiKfzxe2DRw40PW0mt2OHTs0depUpaamyufzacOGDWHPG2P01FNPKSUlRTExMcrOztbhw4fdTLYZNbYOc+bMuez8mDJlipvJNpO8vDyNGDFCcXFxSkxM1PTp03Xo0KGwMdXV1crNzVX37t0VGxurmTNnqrS01NGMm8f1rMO4ceMuOx8ee+wxRzO+sjYRQK+99poWLVqkJUuW6IMPPtCwYcM0efJknTp1yvXUWtzNN9+skydPhrZ3333X9ZSaXWVlpYYNG6YVK1Zc8fmlS5dq+fLlWrlypd5//3117txZkydPVnV1dQvPtHk1tg6SNGXKlLDz49VXX23BGTa/goIC5ebmavfu3dq8ebPq6uo0adIkVVZWhsYsXLhQb7zxhl5//XUVFBSouLhYM2bMcDjrpnc96yBJjzzySNj5sHTpUkczvgrTBowcOdLk5uaGHtfX15vU1FSTl5fncFYtb8mSJWbYsGGup+GUJLN+/frQ44aGBpOcnGyeffbZ0L6ysjLj9/vNq6++6mCGLePSdTDGmNmzZ5tp06Y5mY8rp06dMpJMQUGBMebiv31UVJR5/fXXQ2M+/vhjI8ns2rXL1TSb3aXrYIwxY8eONT/4wQ/cTeo6tPoroNraWu3bt0/Z2dmhfREREcrOztauXbsczsyNw4cPKzU1VX369NFDDz2kY8eOuZ6SU0VFRSopKQk7PwKBgDIzM2/I82P79u1KTEzUTTfdpHnz5uns2bOup9SsgsGgJKlbt26SpH379qmuri7sfBg4cKB69erVrs+HS9fhS/n5+UpISNDgwYO1ePFiVVVVuZjeVbW6btiXOnPmjOrr65WUlBS2PykpSZ988omjWbmRmZmp1atX66abbtLJkyf19NNPa/To0froo48UFxfnenpOlJSUSNIVz48vn7tRTJkyRTNmzFBGRoaOHDmif/zHf1ROTo527dqlyMhI19Nrcg0NDVqwYIFGjRqlwYMHS7p4PkRHR6tLly5hY9vz+XCldZCkBx98UL1791ZqaqoOHjyoH/7whzp06JDWrVvncLbhWn0A4f/l5OSE/jx06FBlZmaqd+/eWrt2rebOnetwZmgN7r///tCfhwwZoqFDh6pv377avn27Jk6c6HBmzSM3N1cfffTRDfE66LVcbR0effTR0J+HDBmilJQUTZw4UUeOHFHfvn1beppX1Op/BZeQkKDIyMjL7mIpLS1VcnKyo1m1Dl26dNGAAQNUWFjoeirOfHkOcH5crk+fPkpISGiX58f8+fP15ptv6p133gn7/LDk5GTV1taqrKwsbHx7PR+utg5XkpmZKUmt6nxo9QEUHR2t4cOHa+vWraF9DQ0N2rp1q7KyshzOzL1z587pyJEjSklJcT0VZzIyMpScnBx2fpSXl+v999+/4c+PEydO6OzZs+3q/DDGaP78+Vq/fr22bdumjIyMsOeHDx+uqKiosPPh0KFDOnbsWLs6Hxpbhys5cOCAJLWu88H1XRDXY82aNcbv95vVq1eb//mf/zGPPvqo6dKliykpKXE9tRb1+OOPm+3bt5uioiLz3nvvmezsbJOQkGBOnTrlemrNqqKiwuzfv9/s37/fSDLPPfec2b9/v/nss8+MMcb8/Oc/N126dDEbN240Bw8eNNOmTTMZGRnm/PnzjmfetK61DhUVFeaJJ54wu3btMkVFRWbLli3mr//6r03//v1NdXW166k3mXnz5plAIGC2b99uTp48GdqqqqpCYx577DHTq1cvs23bNrN3716TlZVlsrKyHM666TW2DoWFheaf//mfzd69e01RUZHZuHGj6dOnjxkzZozjmYdrEwFkjDG/+tWvTK9evUx0dLQZOXKk2b17t+sptbhZs2aZlJQUEx0dbb7xjW+YWbNmmcLCQtfTanbvvPOOkXTZNnv2bGPMxVuxf/zjH5ukpCTj9/vNxIkTzaFDh9xOuhlcax2qqqrMpEmTTI8ePUxUVJTp3bu3eeSRR9rd/6Rd6e8vyaxatSo05vz58+bv//7vTdeuXU2nTp3Mt771LXPy5El3k24Gja3DsWPHzJgxY0y3bt2M3+83/fr1M//wD/9ggsGg24lfgs8DAgA40epfAwIAtE8EEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAODE/wHAY74t0JzoZwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFKFngDruS3_",
        "outputId": "1bb52756-df48-4416-ca8f-f2d8122d9b3e"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wpNVRoKuUJI",
        "outputId": "b1f42b76-8fcd-4b18-c2fa-69d0e5568221"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preparing DataLoader\n",
        "* DataLoader turns the data into a Python iterbale.\n",
        "* Turning the data into mini batches, which can be computationally efficient\n",
        "* So breaking down to batch size of 44 common batch size is 32) ie 44 images/samples at a time\n",
        "* By doing this, it gives the NN more chances to update its gradients per epoch"
      ],
      "metadata": {
        "id": "U3yyccgEuJwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setting up an hyperparameter of batch size\n",
        "BATCH_SIZE = 44\n",
        "\n",
        "# Turning dataset to iterables\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkRfli6vUq3",
        "outputId": "4ab1e4d9-92a4-4159-c82f-03e08e9b9af6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x786cde0cfbf0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x786d09f095b0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out whats going on\n",
        "print(f\"DataLoaders: {train_dataloader, test_dataloader}\")\n",
        "\n",
        "print(f\"Length of the train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRWhINc1qA6",
        "outputId": "4f1e48cd-b29f-4bd0-ead9-a4d2f190e7e0"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders: (<torch.utils.data.dataloader.DataLoader object at 0x786cde0cfbf0>, <torch.utils.data.dataloader.DataLoader object at 0x786d09f095b0>)\n",
            "Length of the train dataloader: 1364 batches of 44\n",
            "Length of test dataloader: 228 batches of 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZM5FL-E3Qv9",
        "outputId": "b1deafa7-2055-4eb5-81ec-f8cbdbaf2743"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([44, 1, 28, 28]), torch.Size([44]))"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Building a baseline model\n",
        "*  A baseline model is a simple model that gives a meaningfull performance refernce, and can be improved upon experiments\n",
        "* Used a Flatten layer, which converts a multi-dim tensor to a 1D vector so that can be fed into a linear layer (which cant handle multi dimensional data)"
      ],
      "metadata": {
        "id": "oBG0qa1w2Yts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Started by creating a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flattening the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "print(f\"Shape before Flattening: {x.shape}\")\n",
        "print(f\"Shape after Flattening: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnrS-1o-5uaM",
        "outputId": "470d76ce-4703-42de-86bc-a0edb5f9abb1"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before Flattening: torch.Size([1, 28, 28])\n",
            "Shape after Flattening: torch.Size([1, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "qUIoV5aj6uj3"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_1 = FashionMNISTModelV1(\n",
        "    input_shape=784,\n",
        "    hidden_units= 10,\n",
        "    output_shape= len(class_names)\n",
        ")\n",
        "model_1.to(device)\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn8WOY6PmAkJ",
        "outputId": "53de6d64-437b-499b-de0e-ada4b861b4cc"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV1(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA04xiMxolpO",
        "outputId": "7b6a68c6-d322-40f4-c432-9695f3162c28"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_stack.1.weight',\n",
              "              tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
              "                      [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
              "                      [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
              "                      ...,\n",
              "                      [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
              "                      [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
              "                      [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]],\n",
              "                     device='cuda:0')),\n",
              "             ('layer_stack.1.bias',\n",
              "              tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,\n",
              "                       0.0018,  0.0163], device='cuda:0')),\n",
              "             ('layer_stack.2.weight',\n",
              "              tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,\n",
              "                        0.2019,  0.2847],\n",
              "                      [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,\n",
              "                        0.0932, -0.1864],\n",
              "                      [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,\n",
              "                       -0.2877, -0.1792],\n",
              "                      [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,\n",
              "                        0.1030, -0.2715],\n",
              "                      [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,\n",
              "                        0.2252, -0.2160],\n",
              "                      [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,\n",
              "                       -0.1533,  0.0965],\n",
              "                      [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,\n",
              "                       -0.2629,  0.0133],\n",
              "                      [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,\n",
              "                        0.2488, -0.2571],\n",
              "                      [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,\n",
              "                        0.1943,  0.2853],\n",
              "                      [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,\n",
              "                        0.0012, -0.0810]], device='cuda:0')),\n",
              "             ('layer_stack.2.bias',\n",
              "              tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,\n",
              "                      -0.2612, -0.2613], device='cuda:0'))])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up a Loss Function & Optimizer\n",
        "* Loss Function: nn.CrossEntrophyLoss() for multi-class\n",
        "* Optimizer: torch.optim.SGD()"
      ],
      "metadata": {
        "id": "MqL_YjbLnV9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)\n",
        "\n",
        "## Evaluation Metric ie accuracy\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc\n",
        "accuracy_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Tnvbn5N_pS7m",
        "outputId": "6f04ded2-0baa-4db2-9d35-50a10cf816a6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.accuracy_fn(y_true, y_pred)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>accuracy_fn</b><br/>def accuracy_fn(y_true, y_pred)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-899957154.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPB2UtWXsaEk",
        "outputId": "e1af9397-6657-4046-dc93-21f00b7ae898"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPI__r8zsdNb",
        "outputId": "7be8e778-5e04-44a1-8398-b34cb230bb3e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.1\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a training loop\n",
        "1. Loop through epochs\n",
        "2. Loop through batches, perform training steps, calculate the train loss per batch\n",
        "3. Loop through testing batch, perform testing steps, calculate the test loss per batch"
      ],
      "metadata": {
        "id": "5gl2B-nssemM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "## Importing tqdm for pregress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Manual seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Setting up epochs\n",
        "epochs = 4\n",
        "\n",
        "# Training and tetsing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n\")\n",
        "\n",
        "  # Training, and adding a loop through training batches\n",
        "  train_loss = 0\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    model_1.train()\n",
        "\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    ## Forward Pass\n",
        "    y_pred = model_1(X)\n",
        "\n",
        "    ## Loss calculation\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # Accumulate the train loss\n",
        "\n",
        "    ## Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    ## Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Printing out whats goin on\n",
        "    if batch % 300 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Dividing total train loss by length of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  ## Testing Loop\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "\n",
        "      X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "      ## Forward pass\n",
        "      test_pred = model_1(X_test)\n",
        "\n",
        "      ## Loss calculation\n",
        "      loss = loss_fn(test_pred, y_test)\n",
        "      test_loss += loss\n",
        "\n",
        "      ## Accuracy\n",
        "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculating the test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Calculating the accuract average per batch\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  ## Printing out whats goin on\n",
        "  print(f\"\\nTrain Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total_training_time_Model_1: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724,
          "referenced_widgets": [
            "11a7ea948fdc496c90fab9945db15141",
            "863ed87b29d64a7f92ea88134ab20c81",
            "c8055a9a852c4b0ea80f8dd2352d60fe",
            "1411d8d7306847a6b917bab708e43c6a",
            "6c4ffeed693f427696e42e6c7001fb83",
            "f9e36a07b94f47f998a2ac2d53cd97d8",
            "ed97707959d645139d5e3f8f47d85d33",
            "69624477bec6473d9381d7be0fb65eb5",
            "10ad8c3fb1d3404f92e96d4380200ad9",
            "9fee7e8ce0ec48d19e301523207caeff",
            "4c451af947904eb493ff2033c03ee578"
          ]
        },
        "id": "sueRW4MSxeRz",
        "outputId": "d3647412-4929-4106-e8e5-47d6bc58ad62"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11a7ea948fdc496c90fab9945db15141"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4164 | Test Loss: 0.4627, Test Acc: 83.5859%\n",
            "Epoch: 1\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4165 | Test Loss: 0.4627, Test Acc: 83.5859%\n",
            "Epoch: 2\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4164 | Test Loss: 0.4627, Test Acc: 83.5859%\n",
            "Epoch: 3\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4165 | Test Loss: 0.4627, Test Acc: 83.5859%\n",
            "Total_training_time_Model_1: 34.83 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Making predictions, evaluation and getting results of model 1\n"
      ],
      "metadata": {
        "id": "yxlnSd_C2TTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before evaluating, defining a function for reusability\n",
        "* if there are mutliple models, instead of writing evaluation code, we can just reuse the code using a function"
      ],
      "metadata": {
        "id": "ac47rkpWw3hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing the results of model predicting on data_loader.\n",
        "    Arguments:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          ## Fowrad pass (predictions)\n",
        "          y_pred = model(X)\n",
        "\n",
        "          # Loss calculation and accuracy (accumulating)\n",
        "          loss += loss_fn(y_pred, y)\n",
        "          acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculating the test loss and accuracy per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "model_1.to(device)\n",
        "# Calculating model 1 results on test dataset\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device\n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hURte9vi2D1o",
        "outputId": "ae6dd5a6-4966-4059-cfba-bea833fede78"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV1',\n",
              " 'model_loss': 0.4627329111099243,\n",
              " 'model_acc': 83.58585858585863}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Performing the same evaluating and predicting results without a function"
      ],
      "metadata": {
        "id": "tQYd5w-r2hC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "loss, acc = 0, 0\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in test_dataloader:\n",
        "\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    ## Fowrad pass (predictions)\n",
        "    y_pred = model_1(X)\n",
        "\n",
        "    # Loss calculation and accuracy (accumulating)\n",
        "    loss += loss_fn(y_pred, y)\n",
        "    acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "  # Calculating the test loss and accuracy per batch\n",
        "  loss /= len(test_dataloader)\n",
        "  acc /= len(test_dataloader)\n",
        "\n",
        "print(f\"Model: {model_1.__class__.__name__}\")\n",
        "print(f\"Final Test Loss: {loss.item():.4f}\")\n",
        "print(f\"Final Test Acc: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7jyCuhY2TQn",
        "outputId": "24d85bb5-8c6e-48e8-de79-1d0260ae0930"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: FashionMNISTModelV1\n",
            "Final Test Loss: 0.4627\n",
            "Final Test Acc: 83.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qPPQ98Bt2TNa"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "u-ObWWeL3S79",
        "outputId": "49b7d414-1d46-4be5-e1d9-512f9f2ae3ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2u5b4iDY7fiD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Buidling a better model (model_2) with Non-linearity\n",
        "* Creating a model with Linear and Non-linear layers"
      ],
      "metadata": {
        "id": "hLBUv-4y8CKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV2(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(), #Flattens the inputs from multi dim to single vector\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "PPaIo8Ir8wa3"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instnace of model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model_2 = FashionMNISTModelV2(input_shape=784, # From flatten layer output (28 * 28)\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names))\n",
        "\n",
        "model_2.to(device)\n",
        "next(model_2.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOGRQpG48wVd",
        "outputId": "7014526a-ee0c-4c25-f4fc-6ff464e21a54"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC2XIhj28wP9",
        "outputId": "17646f39-7fc4-4e6f-d420-cd3d759bb1b6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV2(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
              "    (4): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function, optimizer and accuracy function"
      ],
      "metadata": {
        "id": "fRoUnd4kAfbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function measures how wrong the model is\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Updates the model params to\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr=0.1)\n",
        "# Accuracy Metric\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "GnL5ACcJAxAI"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a training loop and testing loop\n",
        "* Creating functions for training loop and testing loop ie **train_mode** and **test_mode**"
      ],
      "metadata": {
        "id": "RRtDhPfHD21B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mode(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "  \"\"\"\n",
        "  The above function performs a training step by learning from dataloader\n",
        "  \"\"\"\n",
        "  ## Setting up train_loss and accuracy to 0\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "\n",
        "  ## Adding a loop to loop through training batches\n",
        "  for batch, (X,y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    ## Forward Pass (raw logits will be the outputs)\n",
        "    y_pred = model(X)\n",
        "\n",
        "    ## Loss Calculation, accuracy and Accumulating the train loss, accuracy\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    ## Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## Backward loss\n",
        "    loss.backward()\n",
        "\n",
        "    ## Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # Dividing total train loss and accuracy by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "zK4KSB49qc49"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Loop"
      ],
      "metadata": {
        "id": "xnQ-h9tKz82l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mode(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "  \"\"\"\n",
        "  Perofrms a testing step\n",
        "  \"\"\"\n",
        "  test_loss, test_acc = 0, 0\n",
        "  ## Putting the model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  ## Turning on inference mode\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "\n",
        "      # Target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## Forward Pass\n",
        "      test_pred = model(X)\n",
        "\n",
        "      ## Loss calculation, accuracy and accumulating\n",
        "      test_loss += loss_fn(test_pred, y).item()\n",
        "      test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Adjusting metrics\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Testing Loss: {test_loss:.4f} | Testing Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "3sRzPKtGw3lU"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Epochs\n",
        "epochs = 4\n",
        "\n",
        "## Creating an optimization and eval loop using train_mode() and test_mode()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n\")\n",
        "\n",
        "  train_mode(model = model_2,\n",
        "             data_loader = train_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             accuracy_fn = accuracy_fn,\n",
        "             device = device)\n",
        "\n",
        "  test_mode(model = model_2,\n",
        "            data_loader = test_dataloader,\n",
        "            loss_fn = loss_fn,\n",
        "            accuracy_fn = accuracy_fn,\n",
        "            device = device)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total_training_time_Model_2: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359,
          "referenced_widgets": [
            "203835248ec448ce95cc05a57cfeb34e",
            "5bbb81c41de5459fb170cc906486a1fd",
            "9330abb7c26446959a7ce04190dae912",
            "87efbb252d6c44b1ad16d886f04f53b7",
            "3e0c1d12d6504f0ebbbc1bbade55b304",
            "6baee7f8df2845ff80c31eb706766a31",
            "66ee3cf4e41e4216989a0b15c71f3164",
            "58da439ab5e3416ba8931f3dacc4c82f",
            "e4afcb03c1e94f8c977aab8bbea9d1f8",
            "c9f70c7abf9c47169566c8240e4efc73",
            "b6de5720b070468086f605dc314c8e81"
          ]
        },
        "collapsed": true,
        "id": "iMelwheoz3qu",
        "outputId": "925eb87f-ebe1-4eea-f6af-b12a4d6bf856"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "203835248ec448ce95cc05a57cfeb34e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Training Loss: 0.8438 | Training Accuracy: 68.12%\n",
            "Testing Loss: 0.8872 | Testing Accuracy: 66.69%\n",
            "Epoch: 1\n",
            "\n",
            "Training Loss: 0.8438 | Training Accuracy: 68.13%\n",
            "Testing Loss: 0.8872 | Testing Accuracy: 66.69%\n",
            "Epoch: 2\n",
            "\n",
            "Training Loss: 0.8439 | Training Accuracy: 68.12%\n",
            "Testing Loss: 0.8872 | Testing Accuracy: 66.69%\n",
            "Epoch: 3\n",
            "\n",
            "Training Loss: 0.8438 | Training Accuracy: 68.12%\n",
            "Testing Loss: 0.8872 | Testing Accuracy: 66.69%\n",
            "Total_training_time_Model_2: 40.09 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results = eval_model(model=model_2,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device\n",
        ")\n",
        "model_2_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcpOHCnn3CTy",
        "outputId": "ce8d2d3e-1b66-41c1-c707-e72be3df960f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV2',\n",
              " 'model_loss': 0.8871752023696899,\n",
              " 'model_acc': 66.68660287081335}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5eVdbEfmEJ8H"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "* CNNs are built for images, and instead of looking at the whole image at once, it looks at small patches, learns patterns and builds them up into more complex features.\n",
        "* Convolutions learn patterns like edges, corners from visual data.\n",
        "* Conv Layers has learned kernels(weights), which extracts features from input image that distinguishes inages from one another\n",
        "* Also known as ConvNets\n",
        "### Architecture:\n",
        "* Input Layer\n",
        "* Convolution Layer\n",
        "* Hidden Layer\n",
        "* Pooling Layer\n",
        "* Output Layer\n",
        "### Hyperparams in conv ayer:\n",
        "* kernel_size: A kernel/filter is a small matrix that slides over the image and extracts features (3 x 3 most common kernel size)\n",
        "* stride: stride is how many pixels the kernel moves each step\n",
        "* padding: padding adds extra pixels (usually zeros) around the image border\n"
      ],
      "metadata": {
        "id": "UaKc2fD4EJ5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Building model 3 ie Convolutional Neural Network"
      ],
      "metadata": {
        "id": "abReTzQnEJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a CNN\n",
        "class FashionMNISTModelV3(nn.Module):\n",
        "  \"\"\"\n",
        "  Using a TinyVGG CNN Architecture\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*7*7,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    # print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    # print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ANJWsCXLuaiT"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiating the cnn model\n",
        "model_3 = FashionMNISTModelV3(input_shape = 1,\n",
        "                              hidden_units = 10,\n",
        "                              output_shape = len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "XdpDCDBT9jBL"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 whats going inside *nn.Conv2d()* (Checking out by creating a random input image)\n",
        "* Conv2d is a core operation in CNN.\n",
        "* A small window(kernel/filter) slides over an input image and looks for patterns\n",
        "* At each position multiplies values and sums them\n",
        "* Produces feature map that highlights patterns such as edges, corners, textures, shapes etc"
      ],
      "metadata": {
        "id": "dksgHBaa95nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Creating a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "single_image = images[0]\n",
        "\n",
        "print(f\"Image Batch Shape: {images.shape}\")\n",
        "print(f\"Single Image Shape: {single_image.shape}\")\n",
        "print(f\"Single Image: {single_image}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etnxwpv5AYN7",
        "outputId": "e476428f-ed04-4d2c-a69d-4e830e844a08"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Batch Shape: torch.Size([32, 3, 64, 64])\n",
            "Single Image Shape: torch.Size([3, 64, 64])\n",
            "Single Image: tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
            "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
            "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
            "         ...,\n",
            "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
            "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
            "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
            "\n",
            "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
            "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
            "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
            "         ...,\n",
            "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
            "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
            "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
            "\n",
            "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
            "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
            "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
            "         ...,\n",
            "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
            "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
            "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Creating a single conv2d layer\n",
        "conv_layer = nn.Conv2d(in_channels=3,\n",
        "                       out_channels=10,\n",
        "                       kernel_size=(3,3),\n",
        "                       stride=1,\n",
        "                       padding=0)\n",
        "\n",
        "# Passing the data throuh convolution layer\n",
        "conv_op = conv_layer(single_image)\n",
        "conv_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-p125Ay79fb",
        "outputId": "f8f30770-f513-41b5-9606-7f27c709a978"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n",
              "         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n",
              "         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n",
              "         ...,\n",
              "         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n",
              "         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n",
              "         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n",
              "\n",
              "        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n",
              "         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n",
              "         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n",
              "         ...,\n",
              "         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n",
              "         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n",
              "         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n",
              "\n",
              "        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n",
              "         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n",
              "         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n",
              "         ...,\n",
              "         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n",
              "         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n",
              "         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n",
              "         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n",
              "         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n",
              "         ...,\n",
              "         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n",
              "         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n",
              "         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n",
              "\n",
              "        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n",
              "         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n",
              "         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n",
              "         ...,\n",
              "         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n",
              "         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n",
              "         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n",
              "\n",
              "        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n",
              "         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n",
              "         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n",
              "         ...,\n",
              "         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n",
              "         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n",
              "         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n",
              "       grad_fn=<SqueezeBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_op.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyInUCcyAQ0W",
        "outputId": "62380f57-a7e4-4820-f0d0-f23c19799b60"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 62, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 *nn.MaxPool2d layer* (Checking out MaxPool2d on single_image)\n",
        "* This is a downsampling layer used in CNNs. It shrinks feature maps while keeping the most important activations\n",
        "* Takes and selects a small window of size (2x2), and looks at values of input feature map and selects the most strongest activation survives\n"
      ],
      "metadata": {
        "id": "b-J92mSDAmly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M3UkQ-zCwFd",
        "outputId": "a2730005-c3df-4dff-9318-98e1ecb3cdfd"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing original shape of sample single image first\n",
        "print(f\"Single image original shape: {single_image.shape}\")\n",
        "\n",
        "# Creating a max pool layer\n",
        "max_p_layer = nn.MaxPool2d(kernel_size=(2,2))\n",
        "\n",
        "# Data through conv layer\n",
        "conv_op = conv_layer(single_image)\n",
        "print(f\"Shape after data going through Convolution 2d layer: {conv_op.shape}\")\n",
        "\n",
        "# Data (conv_op data) through max pool layer\n",
        "max_p_op = max_p_layer(conv_op)\n",
        "print(f\"Shape after Conv2D output data passing through Max Pooling layer: {max_p_op.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwL7dLw_C7Zv",
        "outputId": "223cdece-fb50-4f0c-b13e-1ac1dd48f821"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image original shape: torch.Size([3, 64, 64])\n",
            "Shape after data going through Convolution 2d layer: torch.Size([10, 62, 62])\n",
            "Shape after Conv2D output data passing through Max Pooling layer: torch.Size([10, 31, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Heading back to model_3\n",
        "* Creating a Loss function and a optimizer"
      ],
      "metadata": {
        "id": "62SbXNAfEu62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_3.parameters(),\n",
        "                            lr=0.1)"
      ],
      "metadata": {
        "id": "kI2-waalQXUi"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Training and testing loop (using the functions of model_2)\n",
        "* For training step and testing/eval step, used dunctions that were defined while training the model_2\n",
        "* Lets use those function for training model_3"
      ],
      "metadata": {
        "id": "eYxqkjUrUE3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_mode, test_mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REGzRRZWz4Zp",
        "outputId": "1a0d6f7a-db5a-4e6c-e06c-aef5c2c262d7"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<function __main__.train_mode(model: torch.nn.modules.module.Module, data_loader: torch.utils.data.dataloader.DataLoader, loss_fn: torch.nn.modules.module.Module, optimizer: torch.optim.optimizer.Optimizer, accuracy_fn, device: torch.device)>,\n",
              " <function __main__.test_mode(model: torch.nn.modules.module.Module, data_loader: torch.utils.data.dataloader.DataLoader, loss_fn: torch.nn.modules.module.Module, accuracy_fn, device: torch.device)>)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Training and testing steps\n",
        "epochs = 4\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\")\n",
        "\n",
        "  train_mode(model=model_3,\n",
        "             data_loader=train_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             accuracy_fn=accuracy_fn,\n",
        "             device=device)\n",
        "\n",
        "  test_mode(model=model_3,\n",
        "            data_loader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            accuracy_fn=accuracy_fn,\n",
        "            device=device)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Total_training_time Model_3: {end_time - start_time:.2f} seconds\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "daeb473ded494cd3919a534342c88bfe",
            "7e49b0af49674a3994a1eb337238fbe6",
            "ff6c17f583244cef8c152b5aa7362f9f",
            "0d66c3b9ea5647419535ff5bff76da14",
            "5573187b144d44479561f3439ca5330c",
            "165cba0aaa1c4e509d8596ad13f28696",
            "7f2c4271c36a4c4cb6184fc40ed127b8",
            "b3a2701c38d3436e9373a50202489677",
            "d887410b532b4eddbf29020ff2094231",
            "e8cdbb3a58aa455487f960addd94e1b9",
            "065e3f088b474f1490ff5978e7333e59"
          ]
        },
        "id": "kEvzFZTAz9iO",
        "outputId": "6b087387-9f39-462f-8fce-dd268a9ba713"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daeb473ded494cd3919a534342c88bfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Training Loss: 0.2586 | Training Accuracy: 90.48%\n",
            "Testing Loss: 0.2953 | Testing Accuracy: 89.45%\n",
            "Epoch: 1\n",
            "Training Loss: 0.2528 | Training Accuracy: 90.86%\n",
            "Testing Loss: 0.3159 | Testing Accuracy: 88.92%\n",
            "Epoch: 2\n",
            "Training Loss: 0.2467 | Training Accuracy: 90.84%\n",
            "Testing Loss: 0.2805 | Testing Accuracy: 90.03%\n",
            "Epoch: 3\n",
            "Training Loss: 0.2428 | Training Accuracy: 91.13%\n",
            "Testing Loss: 0.2867 | Testing Accuracy: 89.70%\n",
            "Total_training_time Model_3: 43.10 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3_results = eval_model(model=model_3,\n",
        "                            data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn,\n",
        "                             device=device)\n",
        "model_3_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgPLQEOY1inl",
        "outputId": "07d94c1b-727a-472d-cd99-8ff5358b5db9"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV3',\n",
              " 'model_loss': 0.32500308752059937,\n",
              " 'model_acc': 88.18779904306217}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "models_results = pd.DataFrame([\n",
        "    model_1_results,\n",
        "    model_2_results,\n",
        "    model_3_results\n",
        "])\n",
        "models_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "wKJSZLMI6CYp",
        "outputId": "33ddb71b-e076-4478-d790-8903258573fd"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            model_name  model_loss  model_acc\n",
              "0  FashionMNISTModelV1    0.462733  83.585859\n",
              "1  FashionMNISTModelV2    0.887175  66.686603\n",
              "2  FashionMNISTModelV3    0.325003  88.187799"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc02206f-a709-4500-9436-f9508416f83c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>model_loss</th>\n",
              "      <th>model_acc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>FashionMNISTModelV1</td>\n",
              "      <td>0.462733</td>\n",
              "      <td>83.585859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FashionMNISTModelV2</td>\n",
              "      <td>0.887175</td>\n",
              "      <td>66.686603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FashionMNISTModelV3</td>\n",
              "      <td>0.325003</td>\n",
              "      <td>88.187799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc02206f-a709-4500-9436-f9508416f83c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc02206f-a709-4500-9436-f9508416f83c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc02206f-a709-4500-9436-f9508416f83c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_08f011a8-a815-4591-ad58-7f2c60f1fb71\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('models_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_08f011a8-a815-4591-ad58-7f2c60f1fb71 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('models_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "models_results",
              "summary": "{\n  \"name\": \"models_results\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"FashionMNISTModelV1\",\n          \"FashionMNISTModelV2\",\n          \"FashionMNISTModelV3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_loss\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29301827293125293,\n        \"min\": 0.32500308752059937,\n        \"max\": 0.8871752023696899,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4627329111099243,\n          0.8871752023696899,\n          0.32500308752059937\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_acc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.321543891564032,\n        \"min\": 66.68660287081335,\n        \"max\": 88.18779904306217,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          83.58585858585863,\n          66.68660287081335,\n          88.18779904306217\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualzing\n",
        "models_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"bar\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.xlabel(\"model\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "f-9fQJxM6v9t",
        "outputId": "9705f99d-4139-44fe-8565-5a2572076c56"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALR9JREFUeJzt3XlUVeX+x/EPs8ikYKIYKNchh1AxzanJ0tDMtMzSnwkpzTiQpok5pJmoaU5ZpCXaXeJ0HZptsOuYQ2IOpWllpqlg96bgiAj790fLczsCysGDh8fer7X2Wp5nP2fv795+o4+bfc52syzLEgAAgIHcXV0AAABASRFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACM5enqAkpbfn6+jhw5ooCAALm5ubm6HAAAUAyWZenkyZMKCwuTu3vR112u+yBz5MgRhYeHu7oMAABQAocOHdKNN95Y5PrrPsgEBARI+vNEBAYGurgaAABQHNnZ2QoPD7f9f7wo132QufjrpMDAQIIMAACGudJtIdzsCwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADCWp6sLAACguGoM/djVJVw3Dozv6OoSnIIrMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYnq4uAH+qMfRjV5dw3TgwvqOrSwAAXCNckQEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAY7k0yOTl5WnEiBGKjIyUr6+vatasqVdeeUWWZdnmWJalkSNHqmrVqvL19VXbtm31448/urBqAABQVrg0yEyYMEFvvfWW3njjDe3Zs0cTJkzQxIkTNWPGDNuciRMnavr06UpJSdHmzZvl5+enmJgYnTt3zoWVAwCAssClD438+uuv1blzZ3Xs+OdD/mrUqKEFCxZoy5Ytkv68GjN16lQNHz5cnTt3liS99957Cg0N1YoVK9S9e/cC28zJyVFOTo7tdXZ29jU4EgAA4AouvSLTqlUrrVq1Svv27ZMk7dixQ+vXr1eHDh0kSb/88osyMjLUtm1b23uCgoLUvHlzbdy4sdBtJicnKygoyLaEh4eX/oEAAACXcOkVmaFDhyo7O1t169aVh4eH8vLy9Oqrr6pnz56SpIyMDElSaGio3ftCQ0Nt6y6VlJSkgQMH2l5nZ2cTZgAAuE65NMgsXrxY8+fPV1pamho0aKDt27crMTFRYWFhiouLK9E2fXx85OPj4+RKAQBAWeTSIDN48GANHTrUdq9LVFSUfv31VyUnJysuLk5VqlSRJGVmZqpq1aq292VmZqpx48auKBkAAJQhLr1H5syZM3J3ty/Bw8ND+fn5kqTIyEhVqVJFq1atsq3Pzs7W5s2b1bJly2taKwAAKHtcekWmU6dOevXVVxUREaEGDRro22+/1euvv64+ffpIktzc3JSYmKixY8eqdu3aioyM1IgRIxQWFqYuXbq4snQAAFAGuDTIzJgxQyNGjNBzzz2nY8eOKSwsTE8//bRGjhxpmzNkyBCdPn1aTz31lE6cOKHbbrtNK1euVLly5VxYOQAAKAvcrL9+je51KDs7W0FBQcrKylJgYKCryylSjaEfu7qE68aB8R1dXQKAUsLPSucp6z8ri/v/b561BAAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCxPVxcAoOyqMfRjV5dwXTgwvqOrSwCuW1yRAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMJbLg8zhw4f12GOPKSQkRL6+voqKitLWrVtt6y3L0siRI1W1alX5+vqqbdu2+vHHH11YMQAAKCtcGmSOHz+u1q1by8vLS59++ql2796tyZMnq2LFirY5EydO1PTp05WSkqLNmzfLz89PMTExOnfunAsrBwAAZYGnK3c+YcIEhYeHKzU11TYWGRlp+7NlWZo6daqGDx+uzp07S5Lee+89hYaGasWKFerevXuBbebk5CgnJ8f2Ojs7uxSPAAAAuJJLr8h88MEHatq0qbp166bKlSsrOjpas2fPtq3/5ZdflJGRobZt29rGgoKC1Lx5c23cuLHQbSYnJysoKMi2hIeHl/pxAAAA13BpkNm/f7/eeust1a5dW5999pmeffZZ9e/fX/PmzZMkZWRkSJJCQ0Pt3hcaGmpbd6mkpCRlZWXZlkOHDpXuQQAAAJdx6a+W8vPz1bRpU40bN06SFB0dre+++04pKSmKi4sr0TZ9fHzk4+PjzDIBAEAZ5dIrMlWrVlX9+vXtxurVq6eDBw9KkqpUqSJJyszMtJuTmZlpWwcAAP6+XBpkWrdurb1799qN7du3T9WrV5f0542/VapU0apVq2zrs7OztXnzZrVs2fKa1goAAMoel/5q6fnnn1erVq00btw4PfLII9qyZYtmzZqlWbNmSZLc3NyUmJiosWPHqnbt2oqMjNSIESMUFhamLl26uLJ0AABQBrg0yDRr1kzLly9XUlKSxowZo8jISE2dOlU9e/a0zRkyZIhOnz6tp556SidOnNBtt92mlStXqly5ci6sHAAAlAUuDTKSdP/99+v+++8vcr2bm5vGjBmjMWPGXMOqAACACVz+iAIAAICSIsgAAABjEWQAAICxCDIAAMBYDt3sm5+frzVr1mjdunX69ddfdebMGd1www2Kjo5W27Ztea4RAAC4pop1Rebs2bMaO3aswsPDdd999+nTTz/ViRMn5OHhoZ9++kmjRo1SZGSk7rvvPm3atKm0awYAAJBUzCsyderUUcuWLTV79my1a9dOXl5eBeb8+uuvSktLU/fu3fXSSy/pySefdHqxAAAAf1WsIPP555+rXr16l51TvXp1JSUl6YUXXrA9KwkAAKA0FetXS1cKMX/l5eWlmjVrlrggAACA4irxN/teuHBBb7/9tlavXq28vDy1bt1aCQkJPDoAAABcMyUOMv3799e+ffv00EMPKTc3V++99562bt2qBQsWOLM+AACAIhU7yCxfvlwPPvig7fXnn3+uvXv3ysPDQ5IUExOjFi1aOL9CAACAIhT7C/HmzJmjLl266MiRI5KkJk2a6JlnntHKlSv14YcfasiQIWrWrFmpFQoAAHCpYgeZDz/8UD169NBdd92lGTNmaNasWQoMDNRLL72kESNGKDw8XGlpaaVZKwAAgB2H7pF59NFHFRMToyFDhigmJkYpKSmaPHlyadUGAABwWQ4/a6lChQqaNWuWXnvtNcXGxmrw4ME6d+5cadQGAABwWcUOMgcPHtQjjzyiqKgo9ezZU7Vr11Z6errKly+vRo0a6dNPPy3NOgEAAAoodpCJjY2Vu7u7XnvtNVWuXFlPP/20vL29NXr0aK1YsULJycl65JFHSrNWAAAAO8W+R2br1q3asWOHatasqZiYGEVGRtrW1atXT2vXrtWsWbNKpUgAAIDCFDvI3HLLLRo5cqTi4uL05ZdfKioqqsCcp556yqnFAQAAXE6xf7X03nvvKScnR88//7wOHz6st99+uzTrAgAAuKJiX5GpXr26/vWvf5VmLQAAAA4p1hWZ06dPO7RRR+cDAACURLGCTK1atTR+/HgdPXq0yDmWZemLL75Qhw4dNH36dKcVCAAAUJRi/Wpp9erVGjZsmF5++WU1atRITZs2VVhYmMqVK6fjx49r9+7d2rhxozw9PZWUlKSnn366tOsGAAAoXpC56aabtHTpUh08eFBLlizRunXr9PXXX+vs2bOqVKmSoqOjNXv2bHXo0MH2NGwAAIDS5tCzliIiIjRo0CANGjSotOoBAAAoNoeftQQAAFBWEGQAAICxCDIAAMBYBBkAAGAsggwAADCWw0GmRo0aGjNmjA4ePFga9QAAABSbw0EmMTFRy5Yt0z/+8Q+1a9dOCxcuVE5OTmnUBgAAcFklCjLbt2/Xli1bVK9ePfXr109Vq1ZV3759tW3bttKoEQAAoFAlvkemSZMmmj59uo4cOaJRo0bpnXfeUbNmzdS4cWPNmTNHlmU5s04AAIACHPpm37/Kzc3V8uXLlZqaqi+++EItWrRQfHy8fvvtNw0bNkxffvml0tLSnFkrAACAHYeDzLZt25SamqoFCxbI3d1dsbGxmjJliurWrWub8+CDD6pZs2ZOLRQAAOBSDgeZZs2aqV27dnrrrbfUpUsXeXl5FZgTGRmp7t27O6VAAACAojgcZPbv36/q1atfdo6fn59SU1NLXBQAAEBxOHyz77Fjx7R58+YC45s3b9bWrVudUhQAAEBxOBxkEhISdOjQoQLjhw8fVkJCglOKAgAAKA6Hg8zu3bvVpEmTAuPR0dHavXu3U4oCAAAoDoeDjI+PjzIzMwuMHz16VJ6eJf40NwAAgMMcDjL33nuvkpKSlJWVZRs7ceKEhg0bpnbt2jm1OAAAgMtx+BLKpEmTdMcdd6h69eqKjo6WJG3fvl2hoaH65z//6fQCAQAAiuJwkKlWrZp27typ+fPna8eOHfL19VXv3r3Vo0ePQr9TBgAAoLSU6KYWPz8/PfXUU86uBQAAwCElvjt39+7dOnjwoM6fP283/sADD1x1UQAAAMVRom/2ffDBB7Vr1y65ubnZnnLt5uYmScrLy3NuhQAAAEVw+FNLAwYMUGRkpI4dO6by5cvr+++/19q1a9W0aVOtXr26FEoEAAAonMNXZDZu3KivvvpKlSpVkru7u9zd3XXbbbcpOTlZ/fv317ffflsadQIAABTg8BWZvLw8BQQESJIqVaqkI0eOSJKqV6+uvXv3Orc6AACAy3D4iszNN9+sHTt2KDIyUs2bN9fEiRPl7e2tWbNm6R//+Edp1AgAAFAoh4PM8OHDdfr0aUnSmDFjdP/99+v2229XSEiIFi1a5PQCAQAAiuJwkImJibH9uVatWvrhhx/0xx9/qGLFirZPLgEAAFwLDt0jk5ubK09PT3333Xd248HBwYQYAABwzTkUZLy8vBQREcF3xQAAgDLB4U8tvfTSSxo2bJj++OOP0qgHAACg2By+R+aNN97QTz/9pLCwMFWvXl1+fn5267dt2+a04gAAAC7H4SDTpUuXUigDAADAcQ4HmVGjRpVGHQAAAA5z+B4ZAACAssLhKzLu7u6X/ag1n2gCAADXisNBZvny5Xavc3Nz9e2332revHkaPXq00woDAAC4EoeDTOfOnQuMPfzww2rQoIEWLVqk+Ph4pxQGAABwJU67R6ZFixZatWqVszYHAABwRU4JMmfPntX06dNVrVo1Z2wOAACgWBwOMhUrVlRwcLBtqVixogICAjRnzhy99tprJS5k/PjxcnNzU2Jiom3s3LlzSkhIUEhIiPz9/dW1a1dlZmaWeB8AAOD64vA9MlOmTLH71JK7u7tuuOEGNW/eXBUrVixREd98843efvttNWzY0G78+eef18cff6wlS5YoKChIffv21UMPPaQNGzaUaD8AAOD64nCQefzxx51awKlTp9SzZ0/Nnj1bY8eOtY1nZWXp3XffVVpamu6++25JUmpqqurVq6dNmzapRYsWTq0DAACYx+FfLaWmpmrJkiUFxpcsWaJ58+Y5XEBCQoI6duyotm3b2o2np6crNzfXbrxu3bqKiIjQxo0bi9xeTk6OsrOz7RYAAHB9cjjIJCcnq1KlSgXGK1eurHHjxjm0rYULF2rbtm1KTk4usC4jI0Pe3t6qUKGC3XhoaKgyMjIuW19QUJBtCQ8Pd6gmAABgDoeDzMGDBxUZGVlgvHr16jp48GCxt3Po0CENGDBA8+fPV7ly5Rwto0hJSUnKysqyLYcOHXLatgEAQNnicJCpXLmydu7cWWB8x44dCgkJKfZ20tPTdezYMTVp0kSenp7y9PTUmjVrNH36dHl6eio0NFTnz5/XiRMn7N6XmZmpKlWqFLldHx8fBQYG2i0AAOD65PDNvj169FD//v0VEBCgO+64Q5K0Zs0aDRgwQN27dy/2du655x7t2rXLbqx3796qW7euXnzxRYWHh8vLy0urVq1S165dJUl79+7VwYMH1bJlS0fLBgAA1yGHg8wrr7yiAwcO6J577pGn559vz8/PV2xsrEP3yAQEBOjmm2+2G/Pz81NISIhtPD4+XgMHDlRwcLACAwPVr18/tWzZkk8sAQAASSUIMt7e3lq0aJHGjh2r7du3y9fXV1FRUapevbrTi5syZYrc3d3VtWtX5eTkKCYmRm+++abT9wMAAMzkcJC5qHbt2qpdu7Yza9Hq1avtXpcrV04zZ87UzJkznbofAABwfXD4Zt+uXbtqwoQJBcYnTpyobt26OaUoAACA4nA4yKxdu1b33XdfgfEOHTpo7dq1TikKAACgOBwOMqdOnZK3t3eBcS8vL75FFwAAXFMOB5moqCgtWrSowPjChQtVv359pxQFAABQHA7f7DtixAg99NBD+vnnn20Pc1y1apUWLFhQ6DOYAAAASovDQaZTp05asWKFxo0bp3/961/y9fVVw4YN9eWXX+rOO+8sjRoBAAAKVaKPX3fs2FEdO3YsMP7dd98V+JI7AACA0uLwPTKXOnnypGbNmqVbb71VjRo1ckZNAAAAxVLiILN27VrFxsaqatWqmjRpku6++25t2rTJmbUBAABclkO/WsrIyNDcuXP17rvvKjs7W4888ohycnK0YsUKPrEEAACuuWJfkenUqZNuuukm7dy5U1OnTtWRI0c0Y8aM0qwNAADgsop9RebTTz9V//799eyzzzr9GUsAAAAlUewrMuvXr9fJkyd1yy23qHnz5nrjjTf0n//8pzRrAwAAuKxiB5kWLVpo9uzZOnr0qJ5++mktXLhQYWFhys/P1xdffKGTJ0+WZp0AAAAFOPypJT8/P/Xp00fr16/Xrl27NGjQII0fP16VK1fWAw88UBo1AgAAFOqqvkfmpptu0sSJE/Xbb79pwYIFzqoJAACgWK76C/EkycPDQ126dNEHH3zgjM0BAAAUi1OCDAAAgCsQZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyXBpnk5GQ1a9ZMAQEBqly5srp06aK9e/fazTl37pwSEhIUEhIif39/de3aVZmZmS6qGAAAlCUuDTJr1qxRQkKCNm3apC+++EK5ubm69957dfr0aduc559/Xh9++KGWLFmiNWvW6MiRI3rooYdcWDUAACgrPF2585UrV9q9njt3ripXrqz09HTdcccdysrK0rvvvqu0tDTdfffdkqTU1FTVq1dPmzZtUosWLVxRNgAAKCPK1D0yWVlZkqTg4GBJUnp6unJzc9W2bVvbnLp16yoiIkIbN24sdBs5OTnKzs62WwAAwPWpzASZ/Px8JSYmqnXr1rr55pslSRkZGfL29laFChXs5oaGhiojI6PQ7SQnJysoKMi2hIeHl3bpAADARcpMkElISNB3332nhQsXXtV2kpKSlJWVZVsOHTrkpAoBAEBZ49J7ZC7q27evPvroI61du1Y33nijbbxKlSo6f/68Tpw4YXdVJjMzU1WqVCl0Wz4+PvLx8SntkgEAQBng0isylmWpb9++Wr58ub766itFRkbarb/lllvk5eWlVatW2cb27t2rgwcPqmXLlte6XAAAUMa49IpMQkKC0tLS9P777ysgIMB230tQUJB8fX0VFBSk+Ph4DRw4UMHBwQoMDFS/fv3UsmVLPrEEAABcG2TeeustSdJdd91lN56amqrHH39ckjRlyhS5u7ura9euysnJUUxMjN58881rXCkAACiLXBpkLMu64pxy5cpp5syZmjlz5jWoCAAAmKTMfGoJAADAUQQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxiLIAAAAYxFkAACAsQgyAADAWAQZAABgLIIMAAAwFkEGAAAYiyADAACMRZABAADGIsgAAABjEWQAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjGVEkJk5c6Zq1KihcuXKqXnz5tqyZYurSwIAAGVAmQ8yixYt0sCBAzVq1Cht27ZNjRo1UkxMjI4dO+bq0gAAgIuV+SDz+uuv68knn1Tv3r1Vv359paSkqHz58pozZ46rSwMAAC7m6eoCLuf8+fNKT09XUlKSbczd3V1t27bVxo0bC31PTk6OcnJybK+zsrIkSdnZ2aVb7FXKzznj6hKuG2X979ok9KVz0JPOQ086T1nvy4v1WZZ12XllOsj85z//UV5enkJDQ+3GQ0ND9cMPPxT6nuTkZI0ePbrAeHh4eKnUiLInaKqrKwDs0ZMoi0zpy5MnTyooKKjI9WU6yJREUlKSBg4caHudn5+vP/74QyEhIXJzc3NhZebLzs5WeHi4Dh06pMDAQFeXA9CTKHPoSeexLEsnT55UWFjYZeeV6SBTqVIleXh4KDMz0248MzNTVapUKfQ9Pj4+8vHxsRurUKFCaZX4txQYGMh/oChT6EmUNfSkc1zuSsxFZfpmX29vb91yyy1atWqVbSw/P1+rVq1Sy5YtXVgZAAAoC8r0FRlJGjhwoOLi4tS0aVPdeuutmjp1qk6fPq3evXu7ujQAAOBiZT7IPProo/r99981cuRIZWRkqHHjxlq5cmWBG4BR+nx8fDRq1KgCv7oDXIWeRFlDT157btaVPtcEAABQRpXpe2QAAAAuhyADAACMRZABAADGIsg42erVq+Xm5qYTJ04UOefll19W48aNr1lNf1fF+bu4VI0aNTR16tRSq8kV6Mmyg578H/qy7DC9L//WQebxxx+Xm5tbgeWnn34q1f2+8MILdt+N4wwXG7FixYo6d+6c3bpvvvnGdmyXzm/QoIHy8vLs5leoUEFz5861vb60YXfs2KEHHnhAlStXVrly5VSjRg09+uijOnbsmF5++eVCz+lfF+l/5/6ZZ54pcCwJCQlyc3PT448/fvUnpoTOnz+vSpUqafz48YWuf+WVVxQaGqrc3FwdPXpU//d//6c6derI3d1diYmJJd4vPUlPFsWRnly2bJnatWunG264QYGBgWrZsqU+++yzEu+bvqQvi+JIX65fv16tW7dWSEiIfH19VbduXU2ZMuWqa/hbBxlJat++vY4ePWq3REZGluo+/f39FRISUirbDggI0PLly+3G3n33XUVERBQ6f//+/XrvvfeKvf3ff/9d99xzj4KDg/XZZ59pz549Sk1NVVhYmE6fPq0XXnjB7lzeeOONGjNmjN3YReHh4Vq4cKHOnj1rGzt37pzS0tKKrPda8fb21mOPPabU1NQC6yzL0ty5cxUbGysvLy/l5OTohhtu0PDhw9WoUaOr3jc9SU8WxpGeXLt2rdq1a6dPPvlE6enpatOmjTp16qRvv/22xPunL+nLwjjSl35+furbt6/Wrl2rPXv2aPjw4Ro+fLhmzZp1VTX87YOMj4+PqlSpYrdMmzZNUVFR8vPzU3h4uJ577jmdOnXK9p5ff/1VnTp1UsWKFeXn56cGDRrok08+sdtuenq6mjZtqvLly6tVq1bau3evbd2ll0vz8/M1ZswY3XjjjfLx8bF9V85FBw4ckJubm5YtW6Y2bdqofPnyatSoUaFPAI+Li9OcOXNsr8+ePauFCxcqLi6u0OPv16+fRo0aZffE8MvZsGGDsrKy9M477yg6OlqRkZFq06aNpkyZosjISPn7+9udSw8PDwUEBNiNXdSkSROFh4dr2bJltrFly5YpIiJC0dHRdvvNyclR//79bf+yue222/TNN9/Yzfnkk09Up04d+fr6qk2bNjpw4ECB+tevX6/bb79dvr6+Cg8PV//+/XX69OlCjzU+Pl779u3T+vXr7cbXrFmj/fv3Kz4+XtKf/wqbNm2aYmNji/V12ldCT9KTV9uTU6dO1ZAhQ9SsWTPVrl1b48aNU+3atfXhhx8W65wWhr6kL6+2L6Ojo9WjRw81aNBANWrU0GOPPaaYmBitW7euWOe0KH/7IFMYd3d3TZ8+Xd9//73mzZunr776SkOGDLGtT0hIUE5OjtauXatdu3ZpwoQJ8vf3t9vGSy+9pMmTJ2vr1q3y9PRUnz59itzftGnTNHnyZE2aNEk7d+5UTEyMHnjgAf34448FtvnCCy9o+/btqlOnjnr06KELFy7YzenVq5fWrVungwcPSpKWLl2qGjVqqEmTJoXuOzExURcuXNCMGTOKdW6qVKmiCxcuaPny5Vd8tHpx9OnTxy7Jz5kzp9BvbR4yZIiWLl2qefPmadu2bapVq5ZiYmL0xx9/SJIOHTqkhx56SJ06ddL27dv1xBNPaOjQoXbb+Pnnn9W+fXt17dpVO3fu1KJFi7R+/Xr17du30NqioqLUrFkzux92kpSamqpWrVqpbt26V3v4xUZPFo2evHJP5ufn6+TJkwoODnboXFwJfVk0+vLKffntt9/q66+/1p133unQuSjA+huLi4uzPDw8LD8/P9vy8MMPF5i3ZMkSKyQkxPY6KirKevnllwvd5r///W9LkvXll1/axj7++GNLknX27FnLsixr1KhRVqNGjWzrw8LCrFdffdVuO82aNbOee+45y7Is65dffrEkWe+8845t/ffff29Jsvbs2WO33+PHj1tdunSxRo8ebVmWZbVp08aaNm2atXz5cuuvf91/nZ+SkmIFBwdbJ06csCzLsoKCgqzU1FTb3OrVq1tTpkyxvR42bJjl6elpBQcHW+3bt7cmTpxoZWRkFHo+Ln3vRXFxcVbnzp2tY8eOWT4+PtaBAwesAwcOWOXKlbN+//13q3PnzlZcXJxlWZZ16tQpy8vLy5o/f77t/efPn7fCwsKsiRMnWpZlWUlJSVb9+vXt9vHiiy/ajtGyLCs+Pt566qmn7OasW7fOcnd3t/3dXFpvSkqK5e/vb508edKyLMvKzs62ypcvb/d38Vd33nmnNWDAgELXFQc9SU86uycty7ImTJhgVaxY0crMzCxyzuXQl/SlM/uyWrVqlre3t+Xu7m6NGTOm0PPhiL/9FZk2bdpo+/bttmX69On68ssvdc8996hatWoKCAhQr1699N///ldnzpyRJPXv319jx45V69atNWrUKO3cubPAdhs2bGj7c9WqVSVJx44dKzAvOztbR44cUevWre3GW7durT179pRom3369NHcuXO1f/9+bdy4UT179rzsOYiPj1dISIgmTJhw2XkXvfrqq8rIyFBKSooaNGiglJQU1a1bV7t27SrW+//qhhtuUMeOHTV37lylpqaqY8eOqlSpkt2cn3/+Wbm5uXbnyMvLS7feeqvtHO3Zs0fNmze3e9+lDxbdsWOH5s6dK39/f9sSExOj/Px8/fLLL4XW16NHD+Xl5Wnx4sWSpEWLFsnd3V2PPvqow8daXPQkPenMnkxLS9Po0aO1ePFiVa5c2bGT8Rf0JX3prL5ct26dtm7dqpSUFE2dOlULFixw+Hz81d8+yPj5+alWrVq2JScnR/fff78aNmyopUuXKj09XTNnzpT0593ZkvTEE09o//796tWrl3bt2qWmTZsWuNzo5eVl+/PFu8/z8/OvqtbibrNDhw46e/as4uPj1alTpyveLOfp6alXX31V06ZN05EjR4pVS0hIiLp166ZJkyZpz549CgsL06RJkxw4mv+5+MNk3rx5l72sfLVOnTqlp59+2u6H8Y4dO/Tjjz+qZs2ahb4nMDBQDz/8sO2Sbmpqqh555JECl8ediZ6kJ53VkwsXLtQTTzyhxYsXq23btldVK31JXzqrLyMjIxUVFaUnn3xSzz//vF5++eWrqvdvH2QulZ6ervz8fE2ePFktWrRQnTp1Cm3Y8PBwPfPMM1q2bJkGDRqk2bNnl2h/gYGBCgsL04YNG+zGN2zYoPr165dom56enoqNjdXq1auL3ezdunVTgwYNNHr0aIf35+3trZo1axZ5I9iVtG/fXufPn1dubq5iYmIKrK9Zs6a8vb3tzlFubq6++eYb2zmqV6+etmzZYve+TZs22b1u0qSJdu/ebffD+OLi7e1dZH3x8fFav369PvroI3399de2G9euFXqSnrxUcXpywYIF6t27txYsWKCOHTs6dPzFQV/Sl5cqyc/K/Pz8Yt9AXZQy//Tra61WrVrKzc3VjBkz1KlTJ23YsEEpKSl2cxITE9WhQwfVqVNHx48f17///W/Vq1evxPscPHiwRo0apZo1a6px48ZKTU3V9u3bNX/+/BJv85VXXtHgwYMd+uji+PHjC/2P468++ugjLVy4UN27d1edOnVkWZY+/PBDffLJJ4V+/K44PDw8bJc9PTw8Cqz38/PTs88+q8GDBys4OFgRERGaOHGizpw5Y/sP5ZlnntHkyZM1ePBgPfHEE0pPT7f7fgdJevHFF9WiRQv17dtXTzzxhPz8/LR792598cUXeuONN4qs74477lCtWrUUGxurunXrqlWrVgXmbN++XdKf/5L5/ffftX37dnl7e5f4B+xf0ZP05KWu1JNpaWmKi4vTtGnT1Lx5c2VkZEiSfH19nfLJOom+pC8LulJfzpw5UxEREbabf9euXatJkyapf//+JTofF3FF5hKNGjXS66+/rgkTJujmm2/W/PnzlZycbDcnLy9PCQkJqlevntq3b686derozTffLPE++/fvr4EDB2rQoEGKiorSypUr9cEHH6h27dol3qa3t7cqVapk98VOV3L33Xfr7rvvLnB3/1/Vr19f5cuX16BBg9S4cWO1aNFCixcv1jvvvKNevXqVuN7AwEAFBgYWuX78+PHq2rWrevXqpSZNmuinn37SZ599pooVK0qSIiIitHTpUq1YsUKNGjVSSkqKxo0bZ7eNhg0bas2aNdq3b59uv/12RUdHa+TIkQoLC7tsbW5uburTp4+OHz9e5L/aoqOjFR0drfT0dKWlpSk6Olr33Xefg2ehcPQkPXmpK/XkrFmzdOHCBSUkJKhq1aq2ZcCAASU4E4WjL+nLS12pL/Pz85WUlKTGjRuradOmmjlzpiZMmKAxY8aU4Ez8Zb+W5YTPhQEAALgAV2QAAICxCDIAAMBYBBkAAGAsggwAADAWQQYAABiLIAMAAIxFkAEAAMYiyAAAAGMRZABcl+666y4lJiYWe/7cuXNVoUKFUqsHQOkgyAAAAGMRZAAAgLEIMgCuqbvuukv9+vVTYmKiKlasqNDQUM2ePVunT59W7969FRAQoFq1aunTTz+1vWfNmjW69dZb5ePjo6pVq2ro0KF2D+w7ffq0YmNj5e/vr6pVq2ry5MkF9puTk6MXXnhB1apVk5+fn5o3b67Vq1dfi0MGUIoIMgCuuXnz5qlSpUrasmWL+vXrp2effVbdunVTq1attG3bNt17773q1auXzpw5o8OHD+u+++5Ts2bNtGPHDr311lt69913NXbsWNv2Bg8erDVr1uj999/X559/rtWrV2vbtm12++zbt682btyohQsXaufOnerWrZvat2+vH3/88VofPgBnsgDgGrrzzjut2267zfb6woULlp+fn9WrVy/b2NGjRy1J1saNG61hw4ZZN910k5Wfn29bP3PmTMvf39/Ky8uzTp48aXl7e1uLFy+2rf/vf/9r+fr6WgMGDLAsy7J+/fVXy8PDwzp8+LBdLffcc4+VlJRkWZZlpaamWkFBQaVwxABKk6ergxSAv5+GDRva/uzh4aGQkBBFRUXZxkJDQyVJx44d0549e9SyZUu5ubnZ1rdu3VqnTp3Sb7/9puPHj+v8+fNq3ry5bX1wcLBuuukm2+tdu3YpLy9PderUsasjJydHISEhTj8+ANcOQQbANefl5WX32s3NzW7sYmjJz893yv5OnTolDw8Ppaeny8PDw26dv7+/U/YBwDUIMgDKtHr16mnp0qWyLMsWcDZs2KCAgADdeOONCg4OlpeXlzZv3qyIiAhJ0vHjx7Vv3z7deeedkqTo6Gjl5eXp2LFjuv322112LACcj5t9AZRpzz33nA4dOqR+/frphx9+0Pvvv69Ro0Zp4MCBcnd3l7+/v+Lj4zV48GB99dVX+u677/T444/L3f1/P97q1Kmjnj17KjY2VsuWLdMvv/yiLVu2KDk5WR9//LELjw7A1eKKDIAyrVq1avrkk080ePBgNWrUSMHBwYqPj9fw4cNtc1577TWdOnVKnTp1UkBAgAYNGqSsrCy77aSmpmrs2LEaNGiQDh8+rEqVKqlFixa6//77r/UhAXAiN8uyLFcXAQAAUBL8agkAABiLIAMAAIxFkAEAAMYiyAAAAGMRZAAAgLEIMgAAwFgEGQAAYCyCDAAAMBZBBgAAGIsgAwAAjEWQAQAAxvp/HGqN1XONcaAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t2KIK21ACEmp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}