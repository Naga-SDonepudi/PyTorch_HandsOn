{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNCWVwcDs2S17ozJ02M9wBY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naga-SDonepudi/PyTorch_HandsOn/blob/main/04_Computer_Vision_and_CNN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computer Vision using PyTorch"
      ],
      "metadata": {
        "id": "zJcsUcFVkXRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries\n",
        "* **torchvision** : base domain library with pre trained models for computer vision (consists of popular datasets, model architectures, and common image transformations for computer vision)\n",
        "* Another different libraries are:\n",
        "  * **torchvision.dataset**,\n",
        "  * **torchvision.models**,\n",
        "  * **torchvision.transforms**,\n",
        "  * **torch.utils.data.Dataset**,\n",
        "  * **torch.utils.data.DataLoader**"
      ],
      "metadata": {
        "id": "xsjooF4YkzN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing torch and torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "## Importing matplotlib for visualizing and plotting\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-onzPj2emJxX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print(torchvision.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL9rPuNoo-Gh",
        "outputId": "6e4ec7fb-365e-47b5-e917-010507c7198e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cu126\n",
            "0.24.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aim is to build a computer vision model to find patterns in training data, and use the model to predict on test data\n",
        "### 1. Loading a Dataset\n",
        "* Importing a FashionMNIST dataset from torchvision.datasets\n",
        "* The datasets found in torchvision.datasets are splitted to training and test sets"
      ],
      "metadata": {
        "id": "HJLlmTu9pKWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Train Data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # where to download data to\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=None\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiSrciuesBs9",
        "outputId": "2b5929dc-af74-4586-e9f5-8bf8d1f1e303"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 9.43MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 210kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.82MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bA2xJUBeldXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYfxCDjhkFIq",
        "outputId": "014e47cb-030a-41e7-d559-6e96422b0ca9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
              "           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
              "           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
              "           0.0157, 0.0000, 0.0000, 0.0118],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
              "           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0471, 0.0392, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
              "           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
              "           0.3020, 0.5098, 0.2824, 0.0588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
              "           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
              "           0.5529, 0.3451, 0.6745, 0.2588],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
              "           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
              "           0.4824, 0.7686, 0.8980, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
              "           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
              "           0.8745, 0.9608, 0.6784, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
              "           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
              "           0.8627, 0.9529, 0.7922, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
              "           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
              "           0.8863, 0.7725, 0.8196, 0.2039],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
              "           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
              "           0.9608, 0.4667, 0.6549, 0.2196],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
              "           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
              "           0.8510, 0.8196, 0.3608, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
              "           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
              "           0.8549, 1.0000, 0.3020, 0.0000],\n",
              "          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
              "           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
              "           0.8784, 0.9569, 0.6235, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
              "           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
              "           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
              "           0.9137, 0.9333, 0.8431, 0.0000],\n",
              "          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
              "           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
              "           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
              "           0.8627, 0.9098, 0.9647, 0.0000],\n",
              "          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
              "           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
              "           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
              "           0.8706, 0.8941, 0.8824, 0.0000],\n",
              "          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
              "           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
              "           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
              "           0.8745, 0.8784, 0.8980, 0.1137],\n",
              "          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
              "           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
              "           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
              "           0.8627, 0.8667, 0.9020, 0.2627],\n",
              "          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
              "           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
              "           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
              "           0.7098, 0.8039, 0.8078, 0.4510],\n",
              "          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
              "           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
              "           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
              "           0.6549, 0.6941, 0.8235, 0.3608],\n",
              "          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
              "           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
              "           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
              "           0.7529, 0.8471, 0.6667, 0.0000],\n",
              "          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
              "           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
              "           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
              "           0.3882, 0.2275, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
              "           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 9)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh5v5cbLkiwa",
        "outputId": "5399c8de-26b1-478c-e6f5-08bb2b214728"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiiUpP3PmvrG",
        "outputId": "7e1c5854-b88a-440e-89e0-3b455fddac09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 9)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Visulazing the data"
      ],
      "metadata": {
        "id": "7A-xXZibnK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "print(f\"Image Shape: {image.shape}\")\n",
        "\n",
        "plt.imshow(image.squeeze(), cmap=\"grey\")\n",
        "plt.title(class_names[label]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Cfd-DLHXp_Nx",
        "outputId": "22ba12e5-4b0e-4c2b-9a21-5785696edb08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Shape: torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFKFngDruS3_",
        "outputId": "5acd05a9-f60c-4306-b7a3-14ff01a743be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wpNVRoKuUJI",
        "outputId": "fa068e42-2f14-4484-92bb-18344135c4f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Preparing DataLoader\n",
        "* DataLoader turns the data into a Python iterbale.\n",
        "* Turning the data into mini batches, which can be computationally efficient\n",
        "* So breaking down to batch size of 44 common batch size is 32) ie 44 images/samples at a time\n",
        "* By doing this, it gives the NN more chances to update its gradients per epoch"
      ],
      "metadata": {
        "id": "U3yyccgEuJwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setting up an hyperparameter of batch size\n",
        "BATCH_SIZE = 44\n",
        "\n",
        "# Turning dataset to iterables\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQkRfli6vUq3",
        "outputId": "c08dcdf0-be41-42fe-945f-5597b81566dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e0a5d8d2540>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e0a5e477500>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out whats going on\n",
        "print(f\"DataLoaders: {train_dataloader, test_dataloader}\")\n",
        "\n",
        "print(f\"Length of the train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRWhINc1qA6",
        "outputId": "059f00f5-2e4f-42b4-e4bb-95727098764d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders: (<torch.utils.data.dataloader.DataLoader object at 0x7e0a5d8d2540>, <torch.utils.data.dataloader.DataLoader object at 0x7e0a5e477500>)\n",
            "Length of the train dataloader: 1364 batches of 44\n",
            "Length of test dataloader: 228 batches of 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZM5FL-E3Qv9",
        "outputId": "0fa30f15-4fe0-47f4-8abd-e8d4554f1dff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([44, 1, 28, 28]), torch.Size([44]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Building a baseline model\n",
        "*  A baseline model is a simple model that gives a meaningfull performance refernce, and can be improved upon experiments\n",
        "* Used a Flatten layer, which converts a multi-dim tensor to a 1D vector so that can be fed into a linear layer (which cant handle multi dimensional data)"
      ],
      "metadata": {
        "id": "oBG0qa1w2Yts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Started by creating a flatten layer\n",
        "flatten_model = nn.Flatten()\n",
        "\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flattening the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "print(f\"Shape before Flattening: {x.shape}\")\n",
        "print(f\"Shape after Flattening: {output.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnrS-1o-5uaM",
        "outputId": "bfbcc039-7ae5-499e-9578-4f4d4087e813"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before Flattening: torch.Size([1, 28, 28])\n",
            "Shape after Flattening: torch.Size([1, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class FashionMNISTModelV1(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "qUIoV5aj6uj3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_1 = FashionMNISTModelV1(\n",
        "    input_shape=784,\n",
        "    hidden_units= 10,\n",
        "    output_shape= len(class_names)\n",
        ")\n",
        "model_1.to(\"cpu\")\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn8WOY6PmAkJ",
        "outputId": "8f33c96b-18ba-4891-8688-285a3164d399"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV1(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA04xiMxolpO",
        "outputId": "28124008-d60f-4308-bd7a-353c9c31f8bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_stack.1.weight',\n",
              "              tensor([[ 0.0273,  0.0296, -0.0084,  ..., -0.0142,  0.0093,  0.0135],\n",
              "                      [-0.0188, -0.0354,  0.0187,  ..., -0.0106, -0.0001,  0.0115],\n",
              "                      [-0.0008,  0.0017,  0.0045,  ..., -0.0127, -0.0188,  0.0059],\n",
              "                      ...,\n",
              "                      [-0.0116,  0.0273, -0.0344,  ...,  0.0176,  0.0283, -0.0011],\n",
              "                      [-0.0230,  0.0257,  0.0291,  ..., -0.0187, -0.0087,  0.0001],\n",
              "                      [ 0.0176, -0.0147,  0.0053,  ..., -0.0336, -0.0221,  0.0205]])),\n",
              "             ('layer_stack.1.bias',\n",
              "              tensor([-0.0093,  0.0283, -0.0033,  0.0255,  0.0017,  0.0037, -0.0302, -0.0123,\n",
              "                       0.0018,  0.0163])),\n",
              "             ('layer_stack.2.weight',\n",
              "              tensor([[ 0.0614, -0.0687,  0.0021,  0.2718,  0.2109,  0.1079, -0.2279, -0.1063,\n",
              "                        0.2019,  0.2847],\n",
              "                      [-0.1495,  0.1344, -0.0740,  0.2006, -0.0475, -0.2514, -0.3130, -0.0118,\n",
              "                        0.0932, -0.1864],\n",
              "                      [ 0.2488,  0.1500,  0.1907,  0.1457, -0.3050, -0.0580,  0.1643,  0.1565,\n",
              "                       -0.2877, -0.1792],\n",
              "                      [ 0.2305, -0.2618,  0.2397, -0.0610,  0.0232,  0.1542,  0.0851, -0.2027,\n",
              "                        0.1030, -0.2715],\n",
              "                      [-0.1596, -0.0555, -0.0633,  0.2302, -0.1726,  0.2654,  0.1473,  0.1029,\n",
              "                        0.2252, -0.2160],\n",
              "                      [-0.2725,  0.0118,  0.1559,  0.1596,  0.0132,  0.3024,  0.1124,  0.1366,\n",
              "                       -0.1533,  0.0965],\n",
              "                      [-0.1184, -0.2555, -0.2057, -0.1909, -0.0477, -0.1324,  0.2905,  0.1307,\n",
              "                       -0.2629,  0.0133],\n",
              "                      [ 0.2727, -0.0127,  0.0513,  0.0863, -0.1043, -0.2047, -0.1185, -0.0825,\n",
              "                        0.2488, -0.2571],\n",
              "                      [ 0.0425, -0.1209, -0.0336, -0.0281, -0.1227,  0.0730,  0.0747, -0.1816,\n",
              "                        0.1943,  0.2853],\n",
              "                      [-0.1310,  0.0645, -0.1171,  0.2168, -0.0245, -0.2820,  0.0736,  0.2621,\n",
              "                        0.0012, -0.0810]])),\n",
              "             ('layer_stack.2.bias',\n",
              "              tensor([-0.0087,  0.1791,  0.2712, -0.0791,  0.1685,  0.1762,  0.2825,  0.2266,\n",
              "                      -0.2612, -0.2613]))])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up a Loss Function & Optimizer\n",
        "* Loss Function: nn.CrossEntrophyLoss() for multi-class\n",
        "* Optimizer: torch.optim.SGD()"
      ],
      "metadata": {
        "id": "MqL_YjbLnV9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(),\n",
        "                            lr=0.1)\n",
        "\n",
        "## Evaluation Metric ie accuracy\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc\n",
        "accuracy_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Tnvbn5N_pS7m",
        "outputId": "04a0bac4-7d08-40e1-8ab5-3d08fd3f6856"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.accuracy_fn(y_true, y_pred)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>accuracy_fn</b><br/>def accuracy_fn(y_true, y_pred)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-899957154.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPB2UtWXsaEk",
        "outputId": "080056b4-a942-44cb-ae2d-1839666d0b65"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPI__r8zsdNb",
        "outputId": "cf24cb3f-9da4-46d1-bca4-b5fc9d5c9ca8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    differentiable: False\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.1\n",
              "    maximize: False\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a training loop\n",
        "1. Loop through epochs\n",
        "2. Loop through batches, perform training steps, calculate the train loss per batch\n",
        "3. Loop through testing batch, perform testing steps, calculate the test loss per batch"
      ],
      "metadata": {
        "id": "5gl2B-nssemM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Importing tqdm for pregress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Manual seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Setting up epochs\n",
        "epochs = 4\n",
        "\n",
        "# Training and tetsing loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n\")\n",
        "\n",
        "  # Training, and adding a loop through training batches\n",
        "  train_loss = 0\n",
        "  for batch, (X, y) in enumerate(train_dataloader):\n",
        "    model_1.train()\n",
        "\n",
        "    ## Forward Pass\n",
        "    y_pred = model_1(X)\n",
        "\n",
        "    ## Loss calculation\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss # Accumulate the train loss\n",
        "\n",
        "    ## Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    ## Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Printing out whats goin on\n",
        "    if batch % 300 == 0:\n",
        "      print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "  # Dividing total train loss by length of train dataloader\n",
        "  train_loss /= len(train_dataloader)\n",
        "\n",
        "  ## Testing Loop\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X_test, y_test in test_dataloader:\n",
        "\n",
        "      ## Forward pass\n",
        "      test_pred = model_1(X_test)\n",
        "\n",
        "      ## Loss calculation\n",
        "      loss = loss_fn(test_pred, y_test)\n",
        "      test_loss += loss\n",
        "\n",
        "      ## Accuracy\n",
        "      test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Calculating the test loss average per batch\n",
        "    test_loss /= len(test_dataloader)\n",
        "\n",
        "    # Calculating the accuract average per batch\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  ## Printing out whats goin on\n",
        "  print(f\"\\nTrain Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706,
          "referenced_widgets": [
            "f6ff0064d19242d9801de5e9731403b6",
            "91deabadaa984893a8de48205b7d68a3",
            "c0f4b5b947d24bf98815e0fbd5fc1109",
            "2305e3446fa94de1a6a3f741155a46be",
            "e276d45e403e46d9be9d64e7a30f236b",
            "4fe3cea3b5274c3e94f9a65fc6f5ecea",
            "0f95cd19aa754cd4bf3bb75ec0312f98",
            "efe3c8a4a48f449e911a29c1053d0f37",
            "be817e246e1c4dcebee2456d61eb11c1",
            "1e30a70fdb4443718d1fae18ae8ece2b",
            "3b989e03536c45b18855619e996b8929"
          ]
        },
        "id": "sueRW4MSxeRz",
        "outputId": "1c45e061-40cd-4f47-dd9d-40655ba6fc31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6ff0064d19242d9801de5e9731403b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.6057 | Test Loss: 0.5236, Test Acc: 81.6221%\n",
            "Epoch: 1\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4767 | Test Loss: 0.5006, Test Acc: 82.2468%\n",
            "Epoch: 2\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4526 | Test Loss: 0.4730, Test Acc: 83.3632%\n",
            "Epoch: 3\n",
            "\n",
            "Looked at 0/60000 samples\n",
            "Looked at 13200/60000 samples\n",
            "Looked at 26400/60000 samples\n",
            "Looked at 39600/60000 samples\n",
            "Looked at 52800/60000 samples\n",
            "\n",
            "Train Loss: 0.4421 | Test Loss: 0.4627, Test Acc: 83.5859%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Making predictions, evaluation and getting results of model 1\n"
      ],
      "metadata": {
        "id": "yxlnSd_C2TTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Before evaluating, defining a function for reusability\n",
        "* if there are mutliple models, instead of writing evaluation code, we can just reuse the code using a function"
      ],
      "metadata": {
        "id": "ac47rkpWw3hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "    \"\"\"\n",
        "    Returns a dictionary containing the results of model predicting on data_loader.\n",
        "    Arguments:\n",
        "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
        "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
        "        loss_fn (torch.nn.Module): The loss function of model.\n",
        "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
        "    \"\"\"\n",
        "    loss, acc = 0, 0\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in data_loader:\n",
        "\n",
        "          ## Fowrad pass (predictions)\n",
        "          y_pred = model(X)\n",
        "\n",
        "          # Loss calculation and accuracy (accumulating)\n",
        "          loss += loss_fn(y_pred, y)\n",
        "          acc += accuracy_fn(y_true=y,\n",
        "                                y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "        # Calculating the test loss and accuracy per batch\n",
        "        loss /= len(data_loader)\n",
        "        acc /= len(data_loader)\n",
        "\n",
        "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "            \"model_loss\": loss.item(),\n",
        "            \"model_acc\": acc}\n",
        "\n",
        "# Calculating model 1 results on test dataset\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn\n",
        ")\n",
        "model_1_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hURte9vi2D1o",
        "outputId": "55b445bb-5415-46c1-bcdd-f3b3ee81e0a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_name': 'FashionMNISTModelV1',\n",
              " 'model_loss': 0.46273303031921387,\n",
              " 'model_acc': 83.58585858585863}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Performing the same evaluating and predicting results without a function"
      ],
      "metadata": {
        "id": "tQYd5w-r2hC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "loss, acc = 0, 0\n",
        "model_1.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in test_dataloader:\n",
        "\n",
        "    ## Fowrad pass (predictions)\n",
        "    y_pred = model_1(X)\n",
        "\n",
        "    # Loss calculation and accuracy (accumulating)\n",
        "    loss += loss_fn(y_pred, y)\n",
        "    acc += accuracy_fn(y_true=y,\n",
        "                         y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "  # Calculating the test loss and accuracy per batch\n",
        "  loss /= len(test_dataloader)\n",
        "  acc /= len(test_dataloader)\n",
        "\n",
        "print(f\"Model: {model_1.__class__.__name__}\")\n",
        "print(f\"Final Test Loss: {loss.item():.4f}\")\n",
        "print(f\"Final Test Acc: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7jyCuhY2TQn",
        "outputId": "0607739a-7c64-4f3e-f293-1d0d31148140"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: FashionMNISTModelV1\n",
            "Final Test Loss: 0.4627\n",
            "Final Test Acc: 83.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "qPPQ98Bt2TNa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "u-ObWWeL3S79",
        "outputId": "5693790e-3963-4dfd-eeff-bf0b946e8356"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2u5b4iDY7fiD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Buidling a better model (model_2) with Non-linearity\n",
        "* Creating a model with Linear and Non-linear layers"
      ],
      "metadata": {
        "id": "hLBUv-4y8CKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV2(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer_stack = nn.Sequential(\n",
        "        nn.Flatten(), #Flattens the inputs from multi dim to single vector\n",
        "        nn.Linear(in_features=input_shape,\n",
        "                  out_features=hidden_units),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "PPaIo8Ir8wa3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an instnace of model\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model_2 = FashionMNISTModelV2(input_shape=784, # From flatten layer output (28 * 28)\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names))\n",
        "\n",
        "model_2.to(device)\n",
        "next(model_2.parameters()).device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOGRQpG48wVd",
        "outputId": "4ae0b575-071a-4e69-9b99-905ec5d0d16f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC2XIhj28wP9",
        "outputId": "8df19476-dafc-4157-f05c-8fd0683ab54d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTModelV2(\n",
              "  (layer_stack): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
              "    (4): ReLU()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function, optimizer and accuracy function"
      ],
      "metadata": {
        "id": "fRoUnd4kAfbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function measures how wrong the model is\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Updates the model params to\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(),\n",
        "                            lr=0.1)\n",
        "# Accuracy Metric\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "GnL5ACcJAxAI"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a training loop and testing loop\n",
        "* Creating functions for training loop and testing loop ie **train_mode** and **test_mode**"
      ],
      "metadata": {
        "id": "RRtDhPfHD21B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mode(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device):\n",
        "  \"\"\"\n",
        "  The above function performs a training step by learning from dataloader\n",
        "  \"\"\"\n",
        "  ## Setting up train_loss and accuracy to 0\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.train()\n",
        "\n",
        "  ## Adding a loop to loop through training batches\n",
        "  for batch, (X,y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    ## Forward Pass (raw logits will be the outputs)\n",
        "    y_pred = model(X)\n",
        "\n",
        "    ## Loss Calculation, accuracy and Accumulating the train loss, accuracy\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    ## Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## Backward loss\n",
        "    loss.backward()\n",
        "\n",
        "    ## Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  # Dividing total train loss and accuracy by length of train dataloader\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "zK4KSB49qc49"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Loop"
      ],
      "metadata": {
        "id": "xnQ-h9tKz82l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mode(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device):\n",
        "  \"\"\"\n",
        "  Perofrms a testing step\n",
        "  \"\"\"\n",
        "  test_loss, test_acc = 0, 0\n",
        "  ## Putting the model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  ## Turning on inference mode\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "\n",
        "      # Target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## Forward Pass\n",
        "      test_pred = model(X)\n",
        "\n",
        "      ## Loss calculation, accuracy and accumulating\n",
        "      test_loss += loss_fn(test_pred, y).item()\n",
        "      test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    # Adjusting metrics\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "    print(f\"Testing Loss: {test_loss:.4f} | Testing Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_loss, test_acc"
      ],
      "metadata": {
        "id": "3sRzPKtGw3lU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Epochs\n",
        "epochs = 4\n",
        "\n",
        "## Creating an optimization and eval loop using train_mode() and test_mode()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n\")\n",
        "\n",
        "  train_mode(model = model_2,\n",
        "             data_loader = train_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             accuracy_fn = accuracy_fn,\n",
        "             device = device)\n",
        "\n",
        "  test_mode(model = model_2,\n",
        "            data_loader = test_dataloader,\n",
        "            loss_fn = loss_fn,\n",
        "            accuracy_fn = accuracy_fn,\n",
        "            device = device)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341,
          "referenced_widgets": [
            "5d0250b5779c492994c9714926f12354",
            "808c289cfc0d49efbd4028617553dc2a",
            "943685a187bf4a779ad46f4e30f32bcd",
            "a10da81b49b3421891ee5682833231b4",
            "cdd80487fba84935ad5a73a4bfc121ca",
            "0d5b837880194322b6b17fdc878db99c",
            "f4369370bfa048bc8bfa110df5e9e7da",
            "cef5d40fd1544050b758c2933778c37c",
            "a19b8bf32caa4ddfb4f372018bfee4a1",
            "b3d1103b41194215bcc65b0ceeab0469",
            "e7eefe352f4c4f8ea37903e89915f33b"
          ]
        },
        "collapsed": true,
        "id": "iMelwheoz3qu",
        "outputId": "304dd95b-5298-47c2-cca6-e36ec1012fe0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d0250b5779c492994c9714926f12354"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Training Loss: 1.1057 | Training Accuracy: 61.13%\n",
            "Testing Loss: 0.9523 | Testing Accuracy: 65.11%\n",
            "Epoch: 1\n",
            "\n",
            "Training Loss: 0.9270 | Training Accuracy: 66.11%\n",
            "Testing Loss: 0.9354 | Testing Accuracy: 65.85%\n",
            "Epoch: 2\n",
            "\n",
            "Training Loss: 0.8869 | Training Accuracy: 67.07%\n",
            "Testing Loss: 0.8944 | Testing Accuracy: 66.67%\n",
            "Epoch: 3\n",
            "\n",
            "Training Loss: 0.8680 | Training Accuracy: 67.73%\n",
            "Testing Loss: 0.8872 | Testing Accuracy: 66.69%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wcpOHCnn3CTy"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5eVdbEfmEJ8H"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "* CNNs are built for images, and instead of looking at the whole image at once, it looks at small patches, learns patterns and builds them up into more complex features.\n",
        "* Convolutions learn patterns like edges, corners from visual data.\n",
        "* Conv Layers has learned kernels(weights), which extracts features from input image that distinguishes inages from one another\n",
        "* Also known as ConvNets\n",
        "### Architecture:\n",
        "* Input Layer\n",
        "* Convolution Layer\n",
        "* Hidden Layer\n",
        "* Pooling Layer\n",
        "* Output Layer\n",
        "### Hyperparams in conv ayer:\n",
        "* kernel_size: A kernel/filter is a small matrix that slides over the image and extracts features (3 x 3 most common kernel size)\n",
        "* stride: stride is how many pixels the kernel moves each step\n",
        "* padding: padding adds extra pixels (usually zeros) around the image border\n"
      ],
      "metadata": {
        "id": "UaKc2fD4EJ5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Building model 3 ie Convolutional Neural Network"
      ],
      "metadata": {
        "id": "abReTzQnEJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a CNN\n",
        "class FashionMNISTModelV3(nn.Module):\n",
        "  \"\"\"\n",
        "  Using a TinyVGG CNN Architecture\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    print(x.shape)\n",
        "    x = self.conv_block_2(x)\n",
        "    print(x.shape)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ANJWsCXLuaiT"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiating the cnn model\n",
        "model_3 = FashionMNISTModelV3(input_shape = 1,\n",
        "                              hidden_units = 10,\n",
        "                              output_shape = len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "XdpDCDBT9jBL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 whats going inside *nn.Conv2d()*"
      ],
      "metadata": {
        "id": "dksgHBaa95nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n"
      ],
      "metadata": {
        "id": "Etnxwpv5AYN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}