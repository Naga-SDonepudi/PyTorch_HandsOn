{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOrbmagc3NzxwyODWwr5yX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naga-SDonepudi/PyTorch_HandsOn/blob/main/6_splitting_code_to_reusable_files.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting code in cells to Reusable scripts\n",
        "* This is termed as going modular using python scripts\n",
        "* The code in cells performing specific functionality like data loading, model builidng and training, model eval, save can be made as Python scripts"
      ],
      "metadata": {
        "id": "tcm9TKq8ODNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data"
      ],
      "metadata": {
        "id": "IXf0eMBXPJSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import requests\n",
        "\n",
        "# Setting a path for data folder\n",
        "data_path = Path(\"data/\")\n",
        "img_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "if img_path.is_dir():\n",
        "  print(f\"{img_path} directory was already there\")\n",
        "else:\n",
        "  print(f\"Creating a {img_path} directory\")\n",
        "  img_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Downloading\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "  request = requests.get(\"https://github.com/Naga-SDonepudi/PyTorch_HandsOn/raw/main/pizza_steak_sushi.zip\")\n",
        "  print(\"Downloaded the zip file\")\n",
        "  f.write(request.content)\n",
        "\n",
        "# Unzipping the data file\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "  print(\"Unzipping the downloaded zip file\")\n",
        "  zip_ref.extractall(img_path)\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkkhkeZhPvUI",
        "outputId": "42c99d19-273f-43e1-c514-01b33aa432cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a data/pizza_steak_sushi directory\n",
            "Downloaded the zip file\n",
            "Unzipping the downloaded zip file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting a path for training and testing data\n",
        "train_dir = img_path / \"train\"\n",
        "test_dir = img_path / \"test\"\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKM6Z5wzTQQM",
        "outputId": "1bda940a-b7b0-4d0f-b5d2-3f42a7ba709b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Data to PyTorch Datsets and DataLoaders"
      ],
      "metadata": {
        "id": "ly9AolrMT_hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Transform\n",
        "data_transform = transforms.Compose([transforms.Resize((64, 64)),\n",
        "                                     transforms.ToTensor()])\n",
        "\n",
        "# ImageFolder for creating datasets\n",
        "train_data = datasets.ImageFolder(root=train_dir,\n",
        "                                  transform=data_transform,\n",
        "                                  target_transform=None)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "train_data, test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn-qIa5rURp1",
        "outputId": "037c0cdd-5f9b-46f0-8e1f-5fd616af6b82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset ImageFolder\n",
              "     Number of datapoints: 225\n",
              "     Root location: data/pizza_steak_sushi/train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                ToTensor()\n",
              "            ),\n",
              " Dataset ImageFolder\n",
              "     Number of datapoints: 75\n",
              "     Root location: data/pizza_steak_sushi/test\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
              "                ToTensor()\n",
              "            ))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KUo4gWsVqZy",
        "outputId": "939231b5-547c-4f1c-9bd0-00a89ea36dc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pizza', 'steak', 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## data to dataloaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1,\n",
        "                              num_workers=1,\n",
        "                              shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=1,\n",
        "                             num_workers=1,\n",
        "                             shuffle=False)\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSk4UahUV3Vj",
        "outputId": "0eef71b7-7193-4767-a783-f277057fb397"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f8ed9015ca0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f8ed93e7fb0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for storing scripts (modular)\n",
        "import os\n",
        "os.makedirs(\"modular_scripts\")"
      ],
      "metadata": {
        "id": "Tw26OQQuZe21"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Script Mode using %%writefile"
      ],
      "metadata": {
        "id": "Thc21b-zWsxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular_scripts/data_setup_script.py\n",
        "\"\"\"\n",
        "Consists of functionality for DataLoaders for classifying image\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_dir: str,\n",
        "    test_dir: str,\n",
        "    transform: transforms.Compose,\n",
        "    batch_size: int,\n",
        "    num_workers: int=NUM_WORKERS):\n",
        "  \"\"\"\n",
        "    Creates training and test dataloaders by taking in a train and test directory, turns them to datasets using ImageFolder and then to PyTorch DataLoaders\n",
        "  Args:\n",
        "    train_dir and test_dir are paths for train and test directories\n",
        "    transforms on train and test data\n",
        "    batch_size means number of samples per batch in each of DataLoaders\n",
        "    num_workers means num of workers per dataloader\n",
        "  Returns:\n",
        "     Returns a tuple of train_dataloader, test_dataloader, class_names (list of target classes)\n",
        "  \"\"\"\n",
        "\n",
        "  # ImageFolder for creating datasets\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "\n",
        "  # Class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # Images to dataloaders\n",
        "  train_dataloader = DataLoader(dataset=train_data,\n",
        "                                batch_size=batch_size,\n",
        "                                num_workers=num_workers,\n",
        "                                shuffle=True,\n",
        "                                pin_memory=True)\n",
        "\n",
        "  test_dataloader = DataLoader(dataset=test_data,\n",
        "                               batch_size=batch_size,\n",
        "                               num_workers=num_workers,\n",
        "                               shuffle=False,\n",
        "                               pin_memory=True)\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NKMbEaTTYAk4",
        "outputId": "0c9b0252-5dc5-4874-95a2-014907cb3091"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular_scripts/data_setup_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the .py file(modular script)"
      ],
      "metadata": {
        "id": "3uMPo3HGL_ES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modular_scripts import data_setup_script"
      ],
      "metadata": {
        "id": "Qm8P-uotNIff"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, test_dataloader, class_names = data_setup_script.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=data_transform,\n",
        "                                                                               batch_size=32)\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn8eExA0OTrI",
        "outputId": "2d90ebb4-16b7-425e-beb5-44178bc25d65"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7f8ed8e59be0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7f8ed8e57ce0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model building and turning to script mode"
      ],
      "metadata": {
        "id": "1mXDzabnOUgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular_scripts/model_script.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\"\"\"\n",
        "PyTorch model code \"\"\"\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    # Convolution block 1\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "\n",
        "    # Convolution block 2\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*16*16,\n",
        "                  out_features=output_shape)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyKQ64fUQdTQ",
        "outputId": "703887a5-f99a-4aa2-d11c-3877164ffc59"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modular_scripts/model_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## peforming forward from model_script\n",
        "from modular_scripts import model_script\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiating\n",
        "torch.manual_seed(42)\n",
        "model_1 = model_script.TinyVGG(input_shape=3,\n",
        "                               hidden_units=10,\n",
        "                               output_shape=len(class_names)).to(device)\n",
        "model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnWbY_GaS-d4",
        "outputId": "ecfccd15-983e-4faa-95a5-9b2891f67dab"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=2560, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. train_mode and test_mode functions to train() step"
      ],
      "metadata": {
        "id": "PcYYyHOxUeaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "## Creating training steps by starting off with defining a function\n",
        "def train_mode(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device=device) -> Tuple[float, float]:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # Setting up evaluation metrics\n",
        "  train_loss, train_acc = 0,0\n",
        "\n",
        "  # Loop through dataloader databatch\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "\n",
        "    # Sending data to target device\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    ## 1. Performing forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    ## 2. Loss calculation and accumulate to train_loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    ## 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## 4. Backward loss propagarion\n",
        "    loss.backward()\n",
        "\n",
        "    ## 5. Optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Calculating accuracy\n",
        "    y_pred_class = y_pred.argmax(dim=1)\n",
        "    train_acc += (y_pred_class==y).sum().item() / len(y_pred)\n",
        "\n",
        "  ## Adjusting metrics for avg loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "QVUDyyYzYdRw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing loop\n",
        "def test_mode(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device=device):\n",
        "\n",
        "  # Eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Test loss and tess accuracy metrics values\n",
        "  test_loss, test_acc = 0,0\n",
        "\n",
        "  # Inference mode\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    ## Loop through dl\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "      # Data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## Forward Pass\n",
        "      test_pred = model(X)\n",
        "\n",
        "      ## Loss\n",
        "      loss = loss_fn(test_pred, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Accuracy\n",
        "      test_pred_labels = test_pred.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
        "\n",
        "  ## Adjusting metrics\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "LwnkaoAuY147"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Combining both to train step\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## Creating a function for train\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module=nn.CrossEntropyLoss(),\n",
        "          epochs: int=5,\n",
        "          device=device):\n",
        "\n",
        "  ## Creating an empty dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": []}\n",
        "\n",
        "  # Looping through the train and tetsing steps\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_mode(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "\n",
        "    test_loss, test_acc = test_mode(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    ## Printing out whats goin on\n",
        "    print(f\"Epoch: {epoch} | Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}% | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    ## Results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "kJNANAoWY-OM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Turning this to a script (train_mode and test_mode functions to train() step then to a .py script)"
      ],
      "metadata": {
        "id": "Tlo1xuK2ZNCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular_scripts/engine_script.py\n",
        "\"\"\" Contains functions for training and testing a model \"\"\"\n",
        "\n",
        "from typing import Tuple, Dict, List\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "## Train Mode Function\n",
        "def train_mode(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device=device) -> Tuple[float, float]:\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  # Setting up evaluation metrics\n",
        "  train_loss, train_acc = 0,0\n",
        "\n",
        "  # Loop through dataloader databatch\n",
        "  for batch, (X,y) in enumerate(dataloader):\n",
        "\n",
        "    # Sending data to target device\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    ## 1. Performing forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    ## 2. Loss calculation and accumulate to train_loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    ## 3. Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## 4. Backward loss propagarion\n",
        "    loss.backward()\n",
        "\n",
        "    ## 5. Optimization\n",
        "    optimizer.step()\n",
        "\n",
        "    ## Calculating accuracy\n",
        "    y_pred_class = y_pred.argmax(dim=1)\n",
        "    train_acc += (y_pred_class==y).sum().item() / len(y_pred)\n",
        "\n",
        "  ## Adjusting metrics for avg loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "\n",
        "## Testing loop\n",
        "def test_mode(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device=device):\n",
        "\n",
        "  # Eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Test loss and tess accuracy metrics values\n",
        "  test_loss, test_acc = 0,0\n",
        "\n",
        "  # Inference mode\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    ## Loop through dl\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "      # Data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      ## Forward Pass\n",
        "      test_pred = model(X)\n",
        "\n",
        "      ## Loss\n",
        "      loss = loss_fn(test_pred, y)\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # Accuracy\n",
        "      test_pred_labels = test_pred.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels == y).sum().item() / len(test_pred_labels))\n",
        "\n",
        "  ## Adjusting metrics\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "## Combining both to train step\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module=nn.CrossEntropyLoss(),\n",
        "          epochs: int=5,\n",
        "          device=device):\n",
        "\n",
        "  ## Creating an empty dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"test_loss\": [],\n",
        "             \"test_acc\": []}\n",
        "\n",
        "  # Looping through the train and tetsing steps\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_mode(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device)\n",
        "\n",
        "    test_loss, test_acc = test_mode(model=model,\n",
        "                                    dataloader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    device=device)\n",
        "\n",
        "    ## Printing out whats goin on\n",
        "    print(f\"Epoch: {epoch} | Training Loss: {train_loss:.4f} | Training Accuracy: {train_acc:.2f}% | Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    ## Results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"test_loss\"].append(test_loss)\n",
        "    results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ztL3vYdZVdO",
        "outputId": "06254b37-a0a8-452f-cd89-95090c061d11"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting modular_scripts/engine_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Utility functions file script"
      ],
      "metadata": {
        "id": "Hdjhvekba2G6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular_scripts/utils_script.py\n",
        "\"\"\"\n",
        "Contains  utility functions for PyTorch model training and saving.\n",
        "\"\"\"\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  \"\"\"Saves a model to a target directory.\n",
        "\n",
        "  Args:\n",
        "    model: A target PyTorch model to save.\n",
        "    target_dir: A directory for saving the model to.\n",
        "    model_name: A filename for the saved model. Should include\n",
        "      either \".pth\" or \".pt\" as the file extension.\n",
        "\n",
        "  Example usage:\n",
        "    save_model(model=model_0,\n",
        "               target_dir=\"models\",\n",
        "               model_name=\"05_going_modular_tingvgg_model.pth\")\n",
        "  \"\"\"\n",
        "  # Create target directory\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True,\n",
        "                        exist_ok=True)\n",
        "\n",
        "  # Create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # Save the model state_dict()\n",
        "  print(f\"[INFO] Saving model to: {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f=model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLBqP0yafHrA",
        "outputId": "81177bfb-9993-4237-a21c-3e012a9b75ef"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular_scripts/utils_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 6. Turning"
      ],
      "metadata": {
        "id": "aYR4YUgvfHob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modular_scripts/train_script.py\n",
        "\"\"\"\n",
        "Trains a PyTorch image classification model using device-agnostic code.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import data_setup_script, engine_script, model_script, utils_script\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "# Setup hyperparameters\n",
        "NUM_EPOCHS = 6\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Setup directories\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "\n",
        "# Setup target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create transforms\n",
        "data_transform = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create DataLoaders with help from data_setup.py\n",
        "train_dataloader, test_dataloader, class_names = data_setup_script.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=data_transform,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Create model with help from model_builder.py\n",
        "model = model_script.TinyVGG(\n",
        "    input_shape=3,\n",
        "    hidden_units=HIDDEN_UNITS,\n",
        "    output_shape=len(class_names)\n",
        ").to(device)\n",
        "\n",
        "# Set loss and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Start training with help from engine.py\n",
        "engine_script.train(model=model,\n",
        "             train_dataloader=train_dataloader,\n",
        "             test_dataloader=test_dataloader,\n",
        "             loss_fn=loss_fn,\n",
        "             optimizer=optimizer,\n",
        "             epochs=NUM_EPOCHS,\n",
        "             device=device)\n",
        "\n",
        "# Save the model with help from utils.py\n",
        "utils_script.save_model(model=model,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=\"06_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeru0ffOguX5",
        "outputId": "bce18a4b-379c-46bf-f458-aa23040b7e8a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modular_scripts/train_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python modular_scripts/train_script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpHTuyrXlODP",
        "outputId": "05fb9411-d939-4578-bebe-74e424b548da"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/6 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "Epoch: 0 | Training Loss: 1.1050 | Training Accuracy: 0.39% | Test Loss: 1.1468 | Test Accuracy: 0.26%\n",
            " 17% 1/6 [00:02<00:11,  2.35s/it]Epoch: 1 | Training Loss: 1.1304 | Training Accuracy: 0.30% | Test Loss: 1.1033 | Test Accuracy: 0.26%\n",
            " 33% 2/6 [00:04<00:08,  2.07s/it]Epoch: 2 | Training Loss: 1.0912 | Training Accuracy: 0.31% | Test Loss: 1.1010 | Test Accuracy: 0.34%\n",
            " 50% 3/6 [00:06<00:07,  2.39s/it]Epoch: 3 | Training Loss: 1.0944 | Training Accuracy: 0.32% | Test Loss: 1.1010 | Test Accuracy: 0.20%\n",
            " 67% 4/6 [00:08<00:04,  2.11s/it]Epoch: 4 | Training Loss: 1.0629 | Training Accuracy: 0.48% | Test Loss: 1.0834 | Test Accuracy: 0.31%\n",
            " 83% 5/6 [00:10<00:01,  1.99s/it]Epoch: 5 | Training Loss: 1.0586 | Training Accuracy: 0.38% | Test Loss: 1.0694 | Test Accuracy: 0.37%\n",
            "100% 6/6 [00:12<00:00,  2.03s/it]\n",
            "[INFO] Saving model to: models/06_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNB1qKhDlakb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}